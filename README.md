# zeropoint-agent

A lightweight application lifecycle orchestrator that treats containerized applications as declarative infrastructure.

## What is zeropoint-agent?

zeropoint-agent is a REST API server that manages Docker-based applications using Terraform as the execution engine. Instead of implementing custom resource management, dependency resolution, or state tracking, it delegates all lifecycle operations to Terraform modules.

**Key Features:**

- **Install, start, stop, and uninstall** containerized applications
- **Link applications** for inter-container communication via DNS
- **Expose applications** to the internet via Envoy reverse proxy with xDS control plane
- **Zero host port conflicts** - apps use service discovery instead of port bindings
- **Terraform-native** - apps are standard Terraform modules using the Docker provider
- **Automatic service discovery** - ports and protocols extracted from app contracts

## Getting Started

### Prerequisites

- [Visual Studio Code](https://code.visualstudio.com/)
- [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
- [Docker](https://docs.docker.com/get-started/get-docker/)

### Development Setup

This project uses VS Code Dev Containers for a consistent development environment.

1. **Clone the repository:**

   ```bash
   git clone https://github.com/zeropoint-os/zeropoint-agent
   cd zeropoint-agent
   ```
2. **Open in VS Code:**

   ```bash
   code .
   ```
3. **Reopen in Container:**

   - Press `F1` or `Cmd/Ctrl+Shift+P`
   - Select: **Dev Containers: Reopen in Container**
   - Wait for the container to build and start
4. **Build/Run the agent:**

   - Use the build task: `Ctrl/Cmd+Shift+B` to build
   - Press `F5` to build & start debugging
   - Or run manually: `go run ./cmd/zeropoint-agent`

The API server will start on `http://localhost:2370` (configurable via `ZEROPOINT_AGENT_PORT` environment variable).

### What's Included in the Dev Container

The dev container provides a complete development environment with:

- **Go 1.25** with gopls, delve debugger
- **Terraform CLI** for app module execution
- **Docker-in-Docker** for managing application containers
- **Development tools**: jq, curl, vim, htop, tree
- **OpenAPI tools**: Swag for generating API documentation

### Install Your First App

```bash
# Install Ollama
curl -X POST http://localhost:2370/apps/install \
  -H "Content-Type: application/json" \
  -d '{
    "source": "github.com/zeropoint-os/ollama",
    "app_id": "ollama"
  }'

# Check status
curl http://localhost:2370/apps | jq
```

## Architecture

### How It Works

1. **Apps are Terraform modules** - Each application is a self-contained `.tf` file that declares Docker resources
2. **Terraform does the heavy lifting** - Dependency graphs, state management, rollback handled by Terraform
3. **REST API orchestrates** - zeropoint-agent invokes `terraform init/apply/destroy` programmatically
4. **Docker DNS for service discovery** - Apps resolve each other by container name (no host ports)
5. **Envoy + xDS for ingress** - External traffic routed via dynamic configuration (no config file reloads)

---

## Architecture

### Module Types

zeropoint manages three types of Terraform modules:

1. **App Modules** (`apps/<app-id>/`)

   - Define application resources: image, container, network, volumes
   - Standalone, no dependencies on other apps
   - Written by developers or imported from repositories
2. **Link Modules** (`links/<from-app>-to-<to-app>/`)

   - Connect two apps by attaching one container to another's network
   - Generated by zeropoint when user creates a link
   - Enable service discovery (DNS resolution between apps)
3. **Exposure Modules** (`exposures/<app-id>/`)

   - Configure reverse proxy to route external traffic to an app
   - Generated when user exposes an app via API
   - Attach reverse proxy to app's network
   - zeropoint-agent configures Envoy routes via xDS API (no Terraform module needed)

### Directory Structure

```
zeropoint-agent/
├── apps/                          # Application modules
│   ├── ollama/
│   │   ├── main.tf
│   │   ├── Dockerfile
│   │   └── terraform.tfstate
│   └── openwebui/
│       ├── main.tf
│       └── terraform.tfstate
├── links/                         # Network link modules (generated)
│   └── openwebui-to-ollama/
│       ├── main.tf
│       └── terraform.tfstate
├── exposures/                     # Reverse proxy exposure tracking (metadata only)
│   └── openwebui/
│       └── metadata.json          # Exposure config (routes managed via xDS)
└── infrastructure/                # Core infrastructure
    └── reverse-proxy/
        ├── main.tf                # Envoy container
        └── terraform.tfstate
```

---

## Application Model

### Example: Ollama App Module

**`apps/ollama/main.tf`:**

```hcl
terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 3.0"
    }
  }
}

variable "app_id" {
  type        = string
  default     = "ollama"
  description = "Unique identifier for this app instance"
}

# Build image from local Dockerfile
resource "docker_image" "ollama" {
  name = "zeropoint-app-${var.app_id}:latest"
  build {
    context    = path.module
    dockerfile = "Dockerfile"
    platform   = "linux/${var.arch}"  # Uses injected arch variable
  }
  keep_locally = true
}

# Create dedicated network
resource "docker_network" "ollama_net" {
  name   = "zeropoint-app-${var.app_id}"
  driver = "bridge"
}

# Main container (no host port binding)
resource "docker_container" "ollama_main" {
  name  = "${var.app_id}-main"
  image = docker_image.ollama.image_id

  networks_advanced {
    name = docker_network.ollama_net.name
  }

  restart = "unless-stopped"

  env = [
    "OLLAMA_HOST=0.0.0.0",
  ]
}

# Outputs for zeropoint
output "main" {
  value       = docker_container.ollama_main
  description = "Main Ollama container"
}

output "main_ports" {
  value = {
    api = {
      port        = 11434
      protocol    = "http"
      transport   = "tcp"
      description = "Ollama API endpoint"
      default     = true
    }
  }
  description = "Service ports for external access"
}
```

### Key Characteristics

- **No host ports**: Container exposes ports internally only
- **Isolated network**: Each app has its own Docker bridge network
- **Self-contained**: All dependencies declared in module
- **Output metadata**: Container name, network name for linking

---

## Service Discovery & Communication

### Internal Communication (App-to-App)

Apps resolve each other by **container name** within Docker networks:

```
OpenWebUI → Ollama: http://ollama-main:11434
```

Docker's built-in DNS automatically resolves container names to IPs within the same network.

### Network Linking

When OpenWebUI needs to communicate with Ollama:

1. Both apps run in their own isolated networks
2. User creates a **link** via zeropoint API
3. zeropoint generates a link module that attaches OpenWebUI container to Ollama's network
4. DNS resolution works automatically

**No configuration changes to apps required.**

---

## Links: Connecting Apps

### Creating a Link

**API Request:**

```http
POST /links
Content-Type: application/json

{
  "from_app": "openwebui",
  "to_app": "ollama",
  "description": "OpenWebUI needs Ollama API"
}
```

**What zeropoint does:**

1. Generate link module at `links/openwebui-to-ollama/main.tf`:

```hcl
terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 3.0"
    }
  }
}

# Reference existing resources via data sources
data "docker_container" "openwebui" {
  name = "openwebui-main"
}

data "docker_network" "ollama_net" {
  name = "zeropoint-app-ollama"
}

# Create network attachment
resource "docker_network_attachment" "openwebui_to_ollama" {
  container_id = data.docker_container.openwebui.id
  network_id   = data.docker_network.ollama_net.id
}

output "link_established" {
  value = "openwebui can now resolve ollama-main:11434"
}
```

2. Run `terraform init && terraform apply -auto-approve` in link module directory

**Result:**

- OpenWebUI container is now attached to both networks:
  - `zeropoint-app-openwebui` (its own)
  - `zeropoint-app-ollama` (Ollama's)
- OpenWebUI can resolve `ollama-main:11434` via DNS
- No restart or configuration change needed

### Deleting a Link

**API Request:**

```http
DELETE /links/openwebui-to-ollama
```

**What zeropoint does:**

1. Run `terraform destroy -auto-approve` in link module directory
2. Remove `links/openwebui-to-ollama/` directory

**Result:**

- Network attachment is removed
- OpenWebUI can no longer reach Ollama
- Clean, reversible operation

---

## Exposures: External Access via Reverse Proxy

### The Problem

Apps don't bind to host ports. How do users access them from outside?

### The Solution

A central **reverse proxy** (Envoy) routes external traffic to apps by hostname. Exposures are opt-in and explicit. zeropoint-agent acts as an xDS control plane, dynamically configuring Envoy routes without generating Terraform modules.

### Architecture

1. **Infrastructure module** runs Envoy container on host ports 80/443
2. **zeropoint-agent xDS control plane** dynamically configures Envoy listeners/clusters/routes
3. **Metadata-only exposures** stored in `exposures/<app-id>/metadata.json` (no Terraform modules)
4. Envoy resolves app containers by DNS name and proxies traffic
5. Agent uses app contract (`main_ports` output) to discover service ports automatically

### Creating an Exposure

**API Request:**

```http
POST /exposures
Content-Type: application/json

{
  "app_id": "openwebui",
  "hostname": "openwebui.zeropoint.local"
}
```

**What zeropoint-agent does:**

1. Read app contract outputs to discover exposed services:

   ```bash
   terraform output -json main_ports
   # Returns: {"web": {"port": 3000, "protocol": "http", ...}}
   ```
2. Attach Envoy to app's network (one-time Terraform operation):

   ```hcl
   # Terraform module to attach Envoy to app network
   resource "docker_network_attachment" "envoy_to_openwebui" {
     container_id = data.docker_container.envoy.id
     network_id   = data.docker_network.openwebui_net.id
   }
   ```
3. Configure Envoy via xDS API (no Terraform, pure Go):

   ```go
   // Agent pushes new configuration to Envoy via xDS gRPC
   snapshot := cache.NewSnapshot(
     version,
     []types.Resource{}, // endpoints
     []types.Resource{  // clusters
       cluster.NewClusterBuilder().
         WithClusterName("openwebui-web").
         WithDNSLookup("openwebui-main", 3000). // Uses container DNS
         Build(),
     },
     []types.Resource{  // routes
       route.NewRouteConfigurationBuilder().
         WithVirtualHost("openwebui.zeropoint.local", "openwebui-web").
         Build(),
     },
     []types.Resource{  // listeners (already configured for 80/443)
       // Listeners are static, routes are dynamic
     },
   )

   xdsServer.SetSnapshot("openwebui", snapshot)
   ```
4. Store exposure metadata:

   ```json
   // exposures/openwebui/metadata.json
   {
     "app_id": "openwebui",
     "hostname": "openwebui.zeropoint.local",
     "services": {
       "web": {
         "port": 3000,
         "protocol": "http",
         "container_target": "openwebui-main:3000"
       }
     },
     "created_at": "2025-12-25T10:00:00Z"
   }
   ```

**Result:**

- Envoy is attached to `zeropoint-app-openwebui` network (Terraform state tracks this)
- Envoy xDS configuration includes route: `openwebui.zeropoint.local` → `openwebui-main:3000`
- External traffic to `https://openwebui.zeropoint.local` reaches OpenWebUI
- No Terraform destroy needed for route updates (xDS is dynamic)
- Ollama remains unexposed (not accessible externally)

### Deleting an Exposure

**API Request:**

```http
DELETE /exposures/openwebui
```

**What zeropoint-agent does:**

1. Remove route from Envoy via xDS API:

   ```go
   // Update snapshot without the deleted route
   snapshot := cache.NewSnapshot(version+1, ...)
   xdsServer.SetSnapshot("openwebui", snapshot)
   ```
2. Optionally detach Envoy from app network (Terraform destroy):

   ```bash
   terraform destroy -auto-approve
   # Removes docker_network_attachment resource
   ```
3. Delete exposure metadata:

   ```bash
   rm -rf exposures/openwebui/
   ```

**Result:**

- Envoy route removed instantly (xDS push)
- App network detachment cleaned up by Terraform
- App is no longer accessible externally

### Benefits of xDS vs Terraform-Generated Config


| Aspect               | xDS Control Plane          | Terraform Config Generation  |
| ---------------------- | ---------------------------- | ------------------------------ |
| **Route updates**    | Instant (gRPC push)        | Slow (terraform apply)       |
| **Config reload**    | Hot reload                 | Container exec reload        |
| **State management** | In-memory + metadata files | Terraform state files        |
| **Scalability**      | Handles 1000s of routes    | Heavy for many small changes |
| **Atomicity**        | Snapshot-based             | Plan/apply lifecycle         |
| **Debugging**        | xDS config dumps           | Parse .tf files              |

**Why Envoy + xDS?**

- **L4/L7 capabilities**: TCP/UDP proxying, HTTP/gRPC routing, WebSocket support
- **Dynamic configuration**: No container restarts, instant route updates
- **Observability**: Built-in metrics, tracing, access logs
- **Future-proof**: Path to service mesh (Istio, Consul Connect)
- **Industry standard**: Same tech powering Kubernetes ingress (Contour, Ambassador)

---

## zeropoint Responsibilities

### What zeropoint DOES

1. **Discover apps**: Scan `apps/` directory for Terraform modules
2. **Invoke Terraform**: Run `terraform init/apply/destroy` programmatically
3. **Generate modules**: Create link modules from API requests (exposures use xDS, not Terraform)
4. **Monitor containers**: Subscribe to Docker events for runtime health tracking
5. **Expose REST API**: Provide HTTP interface for lifecycle operations
6. **Read outputs**: Parse `terraform output -json` to populate API responses and extract service metadata
7. **xDS control plane**: Configure Envoy dynamically via gRPC (listeners, clusters, routes)
8. **Service discovery**: Use `main_ports` output to automatically configure proxy routes without manual port specification

### What zeropoint DOES NOT

1. ❌ Compute dependency graphs (Terraform does this)
2. ❌ Track resource handles manually (Terraform state does this)
3. ❌ Implement custom destroy logic (Terraform provider handles this)
4. ❌ Manage partial rollback (Terraform state locking handles this)
5. ❌ Allocate host ports (apps use service discovery)
6. ❌ Parse devcontainer.json or docker-compose.yml (developers write Terraform)

---

## API Reference

### Apps

#### Install App

```http
POST /apps/install
Content-Type: application/json

{
  "module_path": "/workspaces/zeropoint-agent/apps/ollama",
  "app_id": "ollama",
  "arch": "arm64"  // Optional: override auto-detected architecture
}

Response: 200 OK
{
  "id": "ollama",
  "state": "running",
  "containers": {
    "main": {
      "id": "abc123...",
      "name": "ollama-main",
      "ports": [11434]
    }
  }
}
```

#### List Apps

```http
GET /apps

Response: 200 OK
[
  {
    "id": "ollama",
    "state": "running",
    "module_path": "apps/ollama"
  },
  {
    "id": "openwebui",
    "state": "running",
    "module_path": "apps/openwebui"
  }
]
```

#### Start App

```http
POST /apps/{id}/start

Response: 200 OK
{
  "id": "ollama",
  "state": "running"
}
```

#### Stop App

```http
POST /apps/{id}/stop

Response: 200 OK
{
  "id": "ollama",
  "state": "stopped"
}
```

#### Uninstall App

```http
DELETE /apps/{id}

Response: 204 No Content
```

### Links

#### Create Link

```http
POST /links
Content-Type: application/json

{
  "from_app": "openwebui",
  "to_app": "ollama"
}

Response: 200 OK
{
  "id": "openwebui-to-ollama",
  "from_app": "openwebui",
  "to_app": "ollama",
  "state": "active"
}
```

#### List Links

```http
GET /links

Response: 200 OK
[
  {
    "id": "openwebui-to-ollama",
    "from_app": "openwebui",
    "to_app": "ollama",
    "state": "active"
  }
]
```

#### Delete Link

```http
DELETE /links/{id}

Response: 204 No Content
```

### Exposures

#### Create Exposure

```http
POST /exposures
Content-Type: application/json

{
  "app_id": "openwebui",
  "hostname": "openwebui.zeropoint.local",
  "container_port": 3000
}

Response: 200 OK
{
  "id": "openwebui",
  "app_id": "openwebui",
  "hostname": "openwebui.zeropoint.local",
  "container_port": 3000,
  "external_url": "https://openwebui.zeropoint.local"
}
```

#### List Exposures

```http
GET /exposures

Response: 200 OK
[
  {
    "id": "openwebui",
    "app_id": "openwebui",
    "hostname": "openwebui.zeropoint.local",
    "container_port": 3000
  }
]
```

#### Delete Exposure

```http
DELETE /exposures/{id}

Response: 204 No Content
```

#### Get App Services (Info Endpoint)

```http
GET /apps/{id}/services

Response: 200 OK
{
  "app_id": "openwebui",
  "services": {
    "web": {
      "port": 3000,
      "protocol": "http",
      "transport": "tcp",
      "description": "Web UI",
      "default": true,
      "container_target": "openwebui-main:3000"
    },
    "api": {
      "port": 8080,
      "protocol": "grpc",
      "transport": "tcp",
      "description": "gRPC API",
      "container_target": "openwebui-main:8080"
    }
  }
}
```

**Purpose**: Discover available services from app contract without querying containers. Uses `main_ports` output from Terraform.

---

## Data Model

### State Management Architecture

zeropoint uses a **filesystem-first** approach with no persistent database. All state is derived from:

1. **Filesystem structure** (source of truth for what exists)
2. **Terraform state files** (resource metadata and dependencies)
3. **Docker API** (runtime container states)
4. **In-memory cache** (container → app mapping for event routing)

**Discovery on startup:**

```go
// Scan filesystem for apps
func DiscoverApps() ([]App, error) {
    var apps []App
    entries, _ := os.ReadDir("apps")
  
    for _, entry := range entries {
        if !entry.IsDir() {
            continue
        }
      
        appID := entry.Name()
        modulePath := filepath.Join("apps", appID)
      
        // Read Terraform outputs
        outputs := getTerraformOutputs(modulePath)
      
        // Query Docker for actual state
        containers := make(map[string]ContainerInfo)
        for role, container := range outputs {
            state := inspectContainer(container.id)
            containers[role] = ContainerInfo{
                ID:    container.id,
                Name:  container.name,
                State: state,
            }
        }
      
        apps = append(apps, App{
            ID:         appID,
            ModulePath: modulePath,
            Containers: containers,
        })
    }
    return apps
}
```

**Runtime state (in-memory only):**

```go
type RuntimeState struct {
    sync.RWMutex
  
    // Container ID → App/Role mapping for event routing
    containerRegistry map[string]AppContainer
  
    // App status tracking
    apps map[string]*AppState
}

type AppContainer struct {
    AppID       string
    Role        string  // "main", "db", "worker", etc.
    ContainerID string
    Name        string
}

type AppState struct {
    Status          string            // "running", "stopped", "crashed", "degraded"
    LastCheck       time.Time
    ContainerStates map[string]string // role → state
}
```

**Benefits:**

- ✅ No database synchronization issues
- ✅ Filesystem is authoritative source
- ✅ Terraform state tracks all resources
- ✅ Docker API provides runtime state
- ✅ Fast recovery after restart (rebuild from filesystem)
- ✅ Simple disaster recovery (restore filesystem)

---

## Benefits

### Compared to Custom Resource Graph Model


| Aspect                    | Terraform-Based            | Custom Implementation        |
| --------------------------- | ---------------------------- | ------------------------------ |
| **Dependency resolution** | Built-in, proven           | Custom DAG logic needed      |
| **State management**      | Terraform state            | Custom ephemeral store       |
| **Destroy ordering**      | Automatic reverse graph    | Manual undo DAG generation   |
| **Drift detection**       | `terraform plan`           | Not supported                |
| **Idempotency**           | Native                     | Custom reconciliation        |
| **Provider ecosystem**    | Docker + custom providers  | Hand-code each resource type |
| **Visual editor**         | Read state JSON            | Build custom serialization   |
| **Lock-in**               | None (portable`.tf` files) | Locked to custom internals   |
| **Testing**               | Terraform module tests     | Mock entire graph engine     |

### Compared to Host Port Model


| Aspect              | Service Discovery    | Host Port Allocation       |
| --------------------- | ---------------------- | ---------------------------- |
| **Port conflicts**  | Not possible         | Requires tracking registry |
| **App-to-app comm** | Via DNS (simple)     | Via host IP + port         |
| **Security**        | Only proxy exposed   | All apps exposed to host   |
| **Scaling**         | Easy (auto-register) | Hard (port conflicts)      |
| **Configuration**   | Network attachments  | Port mappings in state     |
| **External access** | Single reverse proxy | Direct per-app             |

---

## App Module Contract

### Overview

zeropoint enforces a minimum contract for app modules to ensure predictable behavior and interoperability. Before applying a module, zeropoint performs a **dry-run validation** (`terraform plan`) to verify compliance. Non-compliant modules are rejected with clear error messages.

### Required Components

#### 1. Required Variables

Every app module MUST accept the following variables:

```hcl
variable "app_id" {
  type        = string
  description = "Unique identifier for this app instance (user-defined, freeform)"
}

variable "network_name" {
  type        = string
  description = "Pre-created Docker network name for this app (managed by zeropoint)"
}

variable "arch" {
  type        = string
  default     = "amd64"
  description = "Target architecture (amd64, arm64, etc.) - injected by zeropoint"
}
```

**Purpose**:

- `app_id`: Unique identifier chosen by user (e.g., `ollama`, `ollama-test`, `alice-nextcloud`). Enables multi-instance deployments with meaningful names.
- `network_name`: zeropoint creates and manages the network lifecycle externally, then injects it into the app module
- `arch`: Target CPU architecture, auto-detected by zeropoint from host (user-overridable via API). Module authors can use this for `platform` selection in builds or ignore it for multi-arch images

**Rationale**: zeropoint creates the network **before** applying the app module. This:

- Separates network lifecycle from container lifecycle
- Makes link module generation simpler (zeropoint already knows the network ID)
- Reduces boilerplate in app modules
- Enables network to exist independently of containers

**Validation**: Module must declare both `app_id` and `network_name` variables. zeropoint will inject these during installation.

#### 2. Required Resources

Every app module MUST declare at minimum:

- **One `docker_container` resource named `${var.app_id}_main`**: The primary application container
- **Additional containers** (optional): Sidecars, databases, workers, etc.

**Note**: Networks are **NOT** declared in app modules. zeropoint creates the network externally and passes it via `var.network_name`.

**Single-Container Example (app_id="ollama"):**

```hcl
resource "docker_container" "ollama_main" {
  name  = "${var.app_id}-main"
  image = docker_image.app.image_id
  
  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }
}
```

**Multi-Container Example (app_id="myapp"):**

```hcl
# Main application container (REQUIRED)
resource "docker_container" "myapp_main" {
  name  = "${var.app_id}-main"
  image = docker_image.app.image_id
  
  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }
  
  env = [
    "DB_HOST=${docker_container.myapp_db.name}:5432",
  ]
}

# Database sidecar (OPTIONAL)
resource "docker_container" "myapp_db" {
  name  = "${var.app_id}-db"
  image = "postgres:15"
  
  networks_advanced {
    name = var.network_name  # Same network as main container
  }
}

# Worker sidecar (OPTIONAL)
resource "docker_container" "myapp_worker" {
  name  = "${var.app_id}-worker"
  image = docker_image.app.image_id
  
  networks_advanced {
    name = var.network_name  # Same network as main container
  }
  
  command = ["worker", "--queue=jobs"]
}
```

**Key Points:**

- The `${var.app_id}_main` container is the **primary entry point** for service discovery
- Terraform resource names MUST use pattern: `${var.app_id}_<role>` (e.g., `ollama_main`, `ollama_db`)
- Runtime container names use hyphens: `${var.app_id}-<role>` (e.g., `ollama-main`, `ollama-db`)
- Additional containers MUST use `var.network_name` (same network for internal communication)
- Additional containers SHOULD follow role naming: `_db`, `_worker`, `_redis`, etc.
- All containers within an app communicate via container names on the shared network
- Network is created by zeropoint **before** applying the app module

**Multi-Instance Support:**

Users can deploy multiple instances of the same app module by providing unique `app_id` values:

- Production: `app_id="ollama"` → resources: `ollama_main`, `ollama_db`
- Testing: `app_id="ollama-test"` → resources: `ollama-test_main`, `ollama-test_db`
- Alice's instance: `app_id="ollama-alice"` → resources: `ollama-alice_main`, `ollama-alice_db`
- Project A: `app_id="projecta-db"` → resources: `projecta-db_main`

**Note**: `app_id` is a freeform unique identifier. Users choose meaningful names for their use case.

**Validation**: `terraform plan` must show at least one `docker_container` resource with Terraform resource name exactly matching `${app_id}_main`.

#### 3. Required Outputs

Every app module MUST expose outputs for containers and service ports.

##### Container Outputs (Required)

All container outputs MUST be `docker_container` resource references, named by role.

**Single-Container Example (app_id="ollama"):**

```hcl
output "main" {
  value       = docker_container.ollama_main
  description = "Main application container"
}
```

**Multi-Container Example (app_id="myapp"):**

```hcl
output "main" {
  value       = docker_container.myapp_main
  description = "Main application container"
}

output "db" {
  value       = docker_container.myapp_db
  description = "Database sidecar container"
}

output "worker" {
  value       = docker_container.myapp_worker
  description = "Background worker container"
}
```

**Contract Rules**:

- Output name = container role (`main`, `db`, `worker`, `cache`, etc.)
- Output value = `docker_container.<resource_name>` (entire resource object)
- `main` output is **REQUIRED** (primary application container)
- Additional outputs are optional (sidecars, workers, etc.)
- All outputs MUST be `docker_container` resources (no primitives, no other resource types)

##### Service Port Metadata (Required for Exposures)

Apps MUST declare a `main_ports` output that describes which ports provide externally-accessible services. This metadata enables zeropoint-agent to automatically configure Envoy routes without requiring users to manually specify ports.

**Structure:**

```hcl
output "main_ports" {
  value = {
    <service_name> = {
      port        = <number>           # Container port number
      protocol    = "<http|grpc|tcp>"  # Application protocol
      transport   = "<tcp|udp>"        # Transport protocol (default: tcp)
      description = "<string>"         # Human-readable description
      default     = <bool>             # Is this the primary service? (optional, one per app)
    }
  }
  description = "Externally-accessible service ports for proxy configuration"
}
```

**Single-Service Example (Ollama API):**

```hcl
output "main_ports" {
  value = {
    api = {
      port        = 11434
      protocol    = "http"
      transport   = "tcp"
      description = "Ollama API endpoint"
      default     = true
    }
  }
}
```

**Multi-Service Example (App with Web UI and API):**

```hcl
output "main_ports" {
  value = {
    web = {
      port        = 3000
      protocol    = "http"
      transport   = "tcp"
      description = "Web UI"
      default     = true  # Primary service
    }
    api = {
      port        = 8080
      protocol    = "grpc"
      transport   = "tcp"
      description = "gRPC API"
    }
    metrics = {
      port        = 9090
      protocol    = "http"
      transport   = "tcp"
      description = "Prometheus metrics endpoint"
    }
  }
}
```

**TCP Service Example (Database):**

```hcl
output "main_ports" {
  value = {
    postgres = {
      port        = 5432
      protocol    = "tcp"
      transport   = "tcp"
      description = "PostgreSQL wire protocol"
      default     = true
    }
  }
}
```

**How zeropoint-agent uses `main_ports`:**

1. **Automatic exposure**: When exposing an app, agent reads `main_ports` to discover services
2. **Default service selection**: If user doesn't specify a service, use `default = true` entry
3. **Protocol-aware routing**:

   - `http`: HTTP/1.1 or HTTP/2 routing with Host header matching
   - `grpc`: HTTP/2 routing with gRPC protocol negotiation
   - `tcp`: L4 TCP proxy (no HTTP parsing)
4. **Multi-service support**: Users can expose individual services from same app:

   ```bash
   POST /exposures {"app_id": "myapp", "service": "web", "hostname": "app.example.com"}
   POST /exposures {"app_id": "myapp", "service": "api", "hostname": "api.example.com"}
   ```
5. **Info endpoints**: Agent can expose metadata about available services:

   ```bash
   GET /apps/myapp/services
   # Returns: {"web": {"port": 3000, "protocol": "http", ...}, ...}
   ```

**Validation Rules:**

- MUST be a map with string keys (service names)
- Each service MUST have `port` (number), `protocol` (string), `description` (string)
- `transport` defaults to `"tcp"` if omitted
- `protocol` MUST be one of: `http`, `grpc`, `tcp`
- At most one service can have `default = true`
- Service names MUST be valid DNS labels (alphanumeric + hyphens, start with letter)

**What zeropoint extracts from outputs:**

```go
// For each container output, zeropoint reads:
containers[role] = ContainerInfo{
    ID:    output.id,           // Container ID for Docker event monitoring
    Name:  output.name,         // Container name for DNS resolution
}

// From main_ports output:
services := make(map[string]ServiceInfo)
for serviceName, svcConfig := range mainPorts {
    services[serviceName] = ServiceInfo{
        Port:        svcConfig.Port,
        Protocol:    svcConfig.Protocol,
        Transport:   svcConfig.Transport,
        Description: svcConfig.Description,
        IsDefault:   svcConfig.Default,
    }
}
```

**Rationale**:

- **No manual port specification**: Users don't need to remember port numbers when exposing apps
- **Self-documenting**: App contract includes service metadata (names, descriptions, protocols)
- **Multi-service support**: Single app can expose multiple endpoints (web UI, API, metrics, admin)
- **Protocol-aware routing**: Envoy can use correct routing logic (HTTP headers vs TCP streams)
- **Info endpoints**: Agent can provide service discovery information without querying containers
- **Convention over configuration**: Developers declare ports once, agent uses them everywhere

#### 4. Naming Conventions

App modules MUST follow these naming patterns:


| Resource                            | Pattern                             | Example                        | Notes                                  |
| ------------------------------------- | ------------------------------------- | -------------------------------- | ---------------------------------------- |
| Main container (Terraform resource) | `${var.app_id}_main`                | `docker_container.ollama_main` | Exact match required                   |
| Main container (runtime name)       | `${var.app_id}-main`                | `ollama-main`                  | DNS-resolvable name (hyphen)           |
| Additional containers (Terraform)   | `${var.app_id}_<role>`              | `docker_container.ollama_db`   | Underscores in resource names          |
| Additional containers (runtime)     | `${var.app_id}-<role>`              | `ollama-db`, `ollama-worker`   | Hyphens in runtime names               |
| Network                             | `zeropoint-app-${var.app_id}`       | `zeropoint-app-ollama`         | Created by zeropoint, passed to module |
| Images                              | `zeropoint-app-${var.app_id}:<tag>` | `zeropoint-app-ollama:latest`  | Optional, can use registry images      |

**Purpose**: Predictable resource discovery for link/exposure generation.

**Multi-Container Naming Example (app_id="myapp"):**

```hcl
resource "docker_container" "myapp_main" {
  name = "${var.app_id}-main"  # Runtime: "myapp-main"
  
  networks_advanced {
    name = var.network_name  # "zeropoint-app-myapp" (created by zeropoint)
  }
}

resource "docker_container" "myapp_db" {
  name = "${var.app_id}-db"    # Runtime: "myapp-db"
  
  networks_advanced {
    name = var.network_name
  }
}

resource "docker_container" "myapp_worker" {
  name = "${var.app_id}-worker" # Runtime: "myapp-worker"
  
  networks_advanced {
    name = var.network_name
  }
}
```

**Multi-Instance Example (app_id="myapp-test"):**

```hcl
resource "docker_container" "myapp-test_main" {
  name = "${var.app_id}-main"  # Runtime: "myapp-test-main"
  
  networks_advanced {
    name = var.network_name  # "zeropoint-app-myapp-test"
  }
}

resource "docker_container" "myapp-test_db" {
  name = "${var.app_id}-db"    # Runtime: "myapp-test-db"
  
  networks_advanced {
    name = var.network_name
  }
}
```

**Note**: Network (`zeropoint-app-${var.app_id}`) is created by zeropoint **before** app module is applied.

**Validation**: zeropoint inspects `terraform plan` JSON output to verify:

- Exactly one container resource with Terraform name `${app_id}_main` (exact match)
- Main container runtime name matches `${app_id}-main` (hyphenated)
- All containers reference `var.network_name` (not a local resource)
- No `docker_network` resources declared in module

#### 5. No Host Port Bindings (Container Resources)

App modules MUST NOT include `ports` blocks in `docker_container` resources:

❌ **Invalid:**

```hcl
resource "docker_container" "main" {
  name  = "${var.app_id}-main"
  image = docker_image.app.image_id
  
  ports {
    internal = 8080
    external = 30001  # ❌ Not allowed
  }
}
```

✅ **Valid:**

```hcl
resource "docker_container" "main" {
  name  = "${var.app_id}-main"
  image = docker_image.app.image_id
  
  # Ports exposed internally via Dockerfile EXPOSE, no host binding
}
```

**Rationale**: Service discovery via DNS replaces host ports. External access is managed by exposure modules.

**Validation**: zeropoint checks `terraform plan` output for `ports` configuration in container resources.

### Validation Process

#### Dry-Run Validation

When a user installs an app via `POST /apps/install`:

```http
POST /apps/install
{
  "module_path": "/workspaces/zeropoint-agent/apps/mycustomapp",
  "app_id": "mycustomapp"
}
```

**zeropoint performs these checks:**

1. **Terraform Init**

   ```bash
   cd /apps/mycustomapp
   terraform init
   ```

   **Success criteria**: Exit code 0, providers downloaded
2. **Terraform Plan (Dry-Run)**

   ```bash
   terraform plan -var="app_id=mycustomapp" -out=/tmp/plan.tfplan
   ```

   **Success criteria**: Exit code 0, plan file generated
3. **Extract Plan JSON**

   ```bash
   terraform show -json /tmp/plan.tfplan > /tmp/plan.json
   ```
4. **Validate Required Resources**

   - Parse `plan.json`
   - Verify `resource_changes` includes:
     - At least one `docker_container` resource with Terraform resource name ending in `_main`
   - Verify module does **not** create `docker_network` resources (zeropoint manages networks)
   - Additional containers (sidecars, etc.) are permitted but not required
5. **Validate Naming Conventions**

   - Check main `docker_container.name` == `"${app_id}-main"`
   - Verify containers use `var.network_name` (not a locally-created network)
6. **Validate No Host Ports**

   - Check `docker_container.ports` is empty or absent
   - Reject if any `external` port bindings exist
7. **Validate Required Outputs**

   - Run `terraform output -json` on plan
   - Verify at least one output named `"main"` exists
   - Verify `"main"` output is a `docker_container` resource
   - Verify all container outputs are `docker_container` resources (reject non-container outputs)
   - Verify `"main_ports"` output exists and is a valid service port map
   - Validate `main_ports` structure:
     - Each service has required fields: `port`, `protocol`, `description`
     - `protocol` is one of: `http`, `grpc`, `tcp`
     - `transport` (if present) is one of: `tcp`, `udp`
     - At most one service has `default = true`
     - Service names are valid DNS labels

**If all checks pass**: Proceed with `terraform apply -auto-approve`

**If any check fails**: Return HTTP 400 with detailed error:

```json
{
  "error": "App module validation failed",
  "details": [
    "Missing required output: 'main' (must reference docker_container resource)",
    "Missing required output: 'main_ports' (must be a map of service port definitions)",
    "Service 'api' in main_ports missing required field: 'protocol'",
    "Service 'web' in main_ports has invalid protocol: 'websocket' (must be http, grpc, or tcp)",
    "Multiple services have default=true (only one allowed): web, api",
    "Output 'config' is not a docker_container resource (non-container outputs not allowed)",
    "Container name 'mycustomapp-app' does not match required pattern '${app_id}-main'",
    "Host port binding detected on container 'mycustomapp-app' (port 8080:30001)"
  ]
}
```

### Validation Implementation (Go)

```go
package validator

import (
    "encoding/json"
    "fmt"
    "os/exec"
    "path/filepath"
)

type PlanOutput struct {
    ResourceChanges []ResourceChange `json:"resource_changes"`
}

type ResourceChange struct {
    Type    string                 `json:"type"`
    Name    string                 `json:"name"`
    Change  Change                 `json:"change"`
}

type Change struct {
    After map[string]interface{} `json:"after"`
}

func ValidateAppModule(modulePath, appID string) error {
    // Step 1: Terraform init
    if err := runTerraform(modulePath, "init"); err != nil {
        return fmt.Errorf("terraform init failed: %w", err)
    }

    // Step 2: Terraform plan
    planFile := filepath.Join("/tmp", fmt.Sprintf("%s-plan.tfplan", appID))
    if err := runTerraformPlan(modulePath, appID, planFile); err != nil {
        return fmt.Errorf("terraform plan failed: %w", err)
    }

    // Step 3: Extract plan JSON
    planJSON, err := extractPlanJSON(planFile)
    if err != nil {
        return fmt.Errorf("failed to extract plan JSON: %w", err)
    }

    // Step 4-6: Validate resources and naming
    if err := validateResources(planJSON, appID); err != nil {
        return err
    }

    // Step 7: Validate outputs
    if err := validateOutputs(modulePath); err != nil {
        return err
    }

    return nil
}

func validateResources(planJSON []byte, appID string) error {
    var plan PlanOutput
    if err := json.Unmarshal(planJSON, &plan); err != nil {
        return err
    }

    var foundMainContainer bool
    var errors []string

    expectedResourceName := fmt.Sprintf("%s_main", appID)
    expectedContainerName := fmt.Sprintf("%s-main", appID)

    for _, rc := range plan.ResourceChanges {
        switch rc.Type {
        case "docker_container":
            // Check if this is the main container (resource name must be exactly "${app_id}_main")
            isMainContainer := rc.Name == expectedResourceName
          
            if isMainContainer {
                foundMainContainer = true
              
                // Check runtime naming for main container
                if name, ok := rc.Change.After["name"].(string); ok {
                    if name != expectedContainerName {
                        errors = append(errors, 
                            fmt.Sprintf("Main container runtime name '%s' does not match required pattern '%s'", 
                                name, expectedContainerName))
                    }
                }
            }

            // Check for host port bindings on ANY container
            if ports, ok := rc.Change.After["ports"].([]interface{}); ok && len(ports) > 0 {
                containerName := rc.Change.After["name"].(string)
                errors = append(errors, 
                    fmt.Sprintf("Host port bindings detected on container '%s' (not allowed)", containerName))
            }

        case "docker_network":
            // App modules should NOT create networks (zeropoint manages them)
            errors = append(errors, 
                fmt.Sprintf("App module must not create docker_network resources (network is provided via var.network_name)"))
        }
    }

    if !foundMainContainer {
        errors = append(errors, fmt.Sprintf("Missing required resource: docker_container.%s (Terraform resource name must be exactly '${app_id}_main')", expectedResourceName))
    }

    if len(errors) > 0 {
        return fmt.Errorf("validation errors:\n- %s", strings.Join(errors, "\n- "))
    }

    return nil
}

func validateOutputs(modulePath string) error {
    cmd := exec.Command("terraform", "output", "-json")
    cmd.Dir = modulePath
  
    output, err := cmd.Output()
    if err != nil {
        return fmt.Errorf("failed to read outputs: %w", err)
    }

    var outputs map[string]interface{}
    if err := json.Unmarshal(output, &outputs); err != nil {
        return err
    }

    var errors []string

    // Check for required 'main' container output
    mainOutput, hasMain := outputs["main"]
    if !hasMain {
        errors = append(errors, "missing required output: 'main' (must reference docker_container resource)")
    } else if !isContainerResource(mainOutput) {
        errors = append(errors, "output 'main' is not a docker_container resource")
    }

    // Check for required 'main_ports' output
    mainPortsOutput, hasMainPorts := outputs["main_ports"]
    if !hasMainPorts {
        errors = append(errors, "missing required output: 'main_ports' (must be a map of service port definitions)")
    } else {
        // Validate main_ports structure
        if portErrors := validateMainPorts(mainPortsOutput); len(portErrors) > 0 {
            errors = append(errors, portErrors...)
        }
    }

    // Verify all non-main_ports outputs are container resources
    for name, val := range outputs {
        if name == "main_ports" {
            continue // Skip main_ports (it's metadata, not a container)
        }
        if !isContainerResource(val) {
            errors = append(errors, 
                fmt.Sprintf("output '%s' is not a docker_container resource (non-container outputs not allowed)", name))
        }
    }

    if len(errors) > 0 {
        return fmt.Errorf("validation errors:\n- %s", strings.Join(errors, "\n- "))
    }

    return nil
}

func validateMainPorts(output interface{}) []string {
    var errors []string
  
    outputMap, ok := output.(map[string]interface{})
    if !ok {
        return []string{"main_ports is not a map"}
    }
  
    value, ok := outputMap["value"].(map[string]interface{})
    if !ok {
        return []string{"main_ports.value is not a map"}
    }
  
    validProtocols := map[string]bool{"http": true, "grpc": true, "tcp": true}
    validTransports := map[string]bool{"tcp": true, "udp": true}
    defaultCount := 0
  
    for serviceName, svcConfig := range value {
        svcMap, ok := svcConfig.(map[string]interface{})
        if !ok {
            errors = append(errors, fmt.Sprintf("service '%s' is not a valid configuration map", serviceName))
            continue
        }
      
        // Validate required fields
        if _, hasPort := svcMap["port"]; !hasPort {
            errors = append(errors, fmt.Sprintf("service '%s' missing required field: 'port'", serviceName))
        }
      
        protocol, hasProtocol := svcMap["protocol"].(string)
        if !hasProtocol {
            errors = append(errors, fmt.Sprintf("service '%s' missing required field: 'protocol'", serviceName))
        } else if !validProtocols[protocol] {
            errors = append(errors, 
                fmt.Sprintf("service '%s' has invalid protocol: '%s' (must be http, grpc, or tcp)", 
                    serviceName, protocol))
        }
      
        if _, hasDesc := svcMap["description"]; !hasDesc {
            errors = append(errors, fmt.Sprintf("service '%s' missing required field: 'description'", serviceName))
        }
      
        // Validate optional transport field
        if transport, hasTransport := svcMap["transport"].(string); hasTransport {
            if !validTransports[transport] {
                errors = append(errors, 
                    fmt.Sprintf("service '%s' has invalid transport: '%s' (must be tcp or udp)", 
                        serviceName, transport))
            }
        }
      
        // Track default services
        if isDefault, ok := svcMap["default"].(bool); ok && isDefault {
            defaultCount++
        }
      
        // Validate service name is a valid DNS label
        if !isValidDNSLabel(serviceName) {
            errors = append(errors, 
                fmt.Sprintf("service name '%s' is not a valid DNS label (alphanumeric + hyphens, start with letter)", 
                    serviceName))
        }
    }
  
    if defaultCount > 1 {
        errors = append(errors, "multiple services have default=true (only one allowed)")
    }
  
    return errors
}

func isValidDNSLabel(s string) bool {
    if len(s) == 0 || len(s) > 63 {
        return false
    }
    if !unicode.IsLetter(rune(s[0])) {
        return false
    }
    for _, ch := range s {
        if !unicode.IsLetter(ch) && !unicode.IsDigit(ch) && ch != '-' {
            return false
        }
    }
    return true
}

func isContainerResource(output interface{}) bool {
    outputMap, ok := output.(map[string]interface{})
    if !ok {
        return false
    }
  
    // Check if output has container-specific attributes
    value, ok := outputMap["value"].(map[string]interface{})
    if !ok {
        return false
    }
  
    // Container resources have 'id', 'name', and 'image' attributes
    _, hasID := value["id"]
    _, hasName := value["name"]
    _, hasImage := value["image"]
  
    return hasID && hasName && hasImage
}
```

### Optional Components

App modules MAY include:

- **Additional containers**: Sidecars, init containers, databases, workers (follow `${app_id}-<role>` naming)
- **Volumes**: For persistent data (`docker_volume` resources)
- **Images from registry**: Pull from Docker Hub instead of building locally
- **Environment variables**: Via `env` blocks in containers
- **Health checks**: Via `healthcheck` blocks
- **Custom variables**: Beyond `app_id`, for user configuration
- **Dependencies between containers**: Use `depends_on` to ensure ordering

**Example of optional components (multi-container app with database):**

```hcl
variable "model_size" {
  type        = string
  default     = "7b"
  description = "Ollama model size to download"
}

# Note: arch variable is already injected by zeropoint, can be used for platform-specific builds
# For multi-arch images pulled from registries, you can ignore var.arch

variable "db_password" {
  type        = string
  sensitive   = true
  description = "PostgreSQL password"
}

# Persistent volume for database
resource "docker_volume" "db_data" {
  name = "zeropoint-app-${var.app_id}-db"
}

# Persistent volume for app data
resource "docker_volume" "app_data" {
  name = "zeropoint-app-${var.app_id}-data"
}

# Database sidecar (starts first)
resource "docker_container" "myapp_db" {
  name  = "${var.app_id}-db"
  image = "postgres:15"

  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }

  volumes {
    volume_name    = docker_volume.db_data.name
    container_path = "/var/lib/postgresql/data"
  }

  env = [
    "POSTGRES_PASSWORD=${var.db_password}",
    "POSTGRES_DB=appdb",
  ]

  healthcheck {
    test     = ["CMD-SHELL", "pg_isready -U postgres"]
    interval = "10s"
    timeout  = "5s"
    retries  = 5
  }
}

# Main application container (waits for db)
resource "docker_container" "myapp_main" {
  name  = "${var.app_id}-main"
  image = docker_image.app.image_id

  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }

  volumes {
    volume_name    = docker_volume.app_data.name
    container_path = "/data"
  }

  env = [
    "DB_HOST=${docker_container.myapp_db.name}",
    "DB_PORT=5432",
    "DB_NAME=appdb",
    "MODEL_SIZE=${var.model_size}",
  ]

  depends_on = [docker_container.myapp_db]  # Wait for database

  healthcheck {
    test     = ["CMD", "curl", "-f", "http://localhost:8080/health"]
    interval = "30s"
    timeout  = "10s"
    retries  = 3
  }
}

# Background worker (optional)
resource "docker_container" "myapp_worker" {
  name  = "${var.app_id}-worker"
  image = docker_image.app.image_id

  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }

  command = ["worker", "--queue=jobs"]

  env = [
    "DB_HOST=${docker_container.myapp_db.name}",
    "DB_PORT=5432",
  ]

  depends_on = [docker_container.myapp_db, docker_container.myapp_main]
}

# Required outputs (all containers as resources)
output "main" {
  value = docker_container.myapp_main
}

output "db" {
  value = docker_container.myapp_db
}

output "worker" {
  value = docker_container.myapp_worker
}
```

**Key Notes for Multi-Container Apps:**

- All containers share the same network (provided via `var.network_name`, created by zeropoint)
- Containers communicate via DNS (e.g., main container reaches db at `${app_id}-db:5432`)
- Use `depends_on` to ensure containers start in the correct order
- Outputs always reference the **main** container (the primary service endpoint)
- Links attach external apps to the shared network, giving them access to all containers

### Non-Compliant Module Examples

#### ❌ Missing Required Output

```hcl
# Missing: output "main"
output "database" {
  value = docker_container.myapp_db
}

output "worker" {
  value = docker_container.myapp_worker
}
```

**Error:**

```
App module validation failed: Missing required output: 'main' (must reference docker_container resource)
```

#### ❌ Non-Container Output

```hcl
output "main" {
  value = docker_container.myapp_main
}

# ❌ Not allowed - outputs must be container resources only
output "admin_password" {
  value = random_password.admin.result
}
```

**Error:**

```
App module validation failed: output 'admin_password' is not a docker_container resource (non-container outputs not allowed)
```

#### ❌ Wrong Resource or Container Naming

```hcl
# Wrong Terraform resource name
resource "docker_container" "main" {  # ❌ Should be "${var.app_id}_main" e.g., "myapp_main"
  name  = "${var.app_id}-main"
  image = "myapp:latest"
}

# Or wrong runtime container name
resource "docker_container" "myapp_main" {
  name  = "myapp-container"  # ❌ Should be "${var.app_id}-main"
  image = "myapp:latest"
}
```

**Errors:**

```
Missing required resource: docker_container.myapp_main (Terraform resource name must be exactly '${app_id}_main')
```

or

```
Main container runtime name 'myapp-container' does not match required pattern 'myapp-main'
```

#### ❌ Host Port Binding

```hcl
resource "docker_container" "myapp_main" {
  name  = "${var.app_id}-main"
  image = "myapp:latest"

  ports {
    internal = 8080
    external = 30001  # ❌ Not allowed
  }
}
```

**Error:**

```
Host port bindings detected on container 'main' (not allowed)
```

#### ❌ Module Creates Network

```hcl
# ❌ App modules must NOT create networks
resource "docker_network" "app_net" {
  name = "zeropoint-app-${var.app_id}"
}

resource "docker_container" "myapp_main" {
  name  = "${var.app_id}-main"
  image = "myapp:latest"
  
  networks_advanced {
    name = docker_network.app_net.name  # Should use var.network_name instead
  }
}
```

**Error:**

```
App module must not create docker_network resources (network is provided via var.network_name)
```

### Contract Enforcement Benefits

1. **Predictable Link Generation**: Container/network names follow known patterns
2. **Service Discovery Works**: Container names guaranteed to be DNS-compatible
3. **No Port Conflicts**: Host port allocation forbidden at app level
4. **API Consistency**: All apps return same output structure
5. **Developer Clarity**: Contract documented, violations clearly reported
6. **Future-Proof**: Can extend contract without breaking existing modules

---

## Implementation Roadmap

### Phase 1: Core Terraform Integration

- [X] Define app module structure
- [X] Test Terraform apply/destroy workflow
- [ ] Implement app module contract validation
- [ ] Implement `hashicorp/terraform-exec` Go SDK integration
- [ ] Add Terraform binary to deployment image
- [ ] Implement basic app lifecycle API (`/apps/install`, `/apps/{id}/start`, etc.)

### Phase 2: Links

- [ ] Design link module template
- [ ] Implement `POST /links` (generate module, apply)
- [ ] Implement `DELETE /links/{id}` (destroy module)
- [ ] Implement filesystem-based link discovery
- [ ] Test multi-container app communication

### Phase 3: Exposures & xDS Control Plane

- [ ] Deploy Envoy infrastructure module
- [ ] Implement xDS control plane using `github.com/envoyproxy/go-control-plane`
- [ ] Design snapshot-based configuration (clusters, routes, listeners)
- [ ] Implement `POST /exposures` (attach Envoy to network, push xDS config)
- [ ] Implement `DELETE /exposures/{id}` (remove route from xDS, detach network)
- [ ] Add service discovery from `main_ports` output
- [ ] Implement metadata-only exposure tracking (no Terraform modules)
- [ ] Support multi-service exposures (single app, multiple hostnames)

### Phase 4: Developer Experience

- [ ] Document app module contract (required outputs, naming conventions)
- [ ] Provide example modules (`apps/examples/`)
- [ ] Add module validation (`terraform validate`)
- [ ] Implement dry-run mode (`terraform plan` without apply)
- [ ] Add graph visualization endpoint (`GET /apps/{id}/graph`)

### Phase 5: Advanced Features

- [ ] Custom Terraform providers for zeropoint-native resources
- [ ] Multi-app composition (parent modules)
- [ ] Backup/restore of Terraform state
- [ ] Remote state backend support (S3, Consul)
- [ ] Web UI for visual graph editing

---

## Example Workflow

### Install Ollama

```bash
curl -X POST http://localhost:2370/apps/install \
  -H "Content-Type: application/json" \
  -d '{
    "module_path": "/apps/ollama",
    "app_id": "ollama"
  }'
```

### Install OpenWebUI

```bash
curl -X POST http://localhost:2370/apps/install \
  -H "Content-Type: application/json" \
  -d '{
    "module_path": "/apps/openwebui",
    "app_id": "openwebui"
  }'
```

### Link OpenWebUI to Ollama

```bash
curl -X POST http://localhost:2370/links \
  -H "Content-Type: application/json" \
  -d '{
    "from_app": "openwebui",
    "to_app": "ollama"
  }'
```

**Result:** OpenWebUI can now call `http://ollama-main:11434`

### Expose OpenWebUI to the World

```bash
curl -X POST http://localhost:2370/exposures \
  -H "Content-Type: application/json" \
  -d '{
    "app_id": "openwebui",
    "hostname": "openwebui.zeropoint.local",
    "container_port": 3000
  }'
```

**Result:** Visit `https://openwebui.zeropoint.local` in browser

### Unlink OpenWebUI from Ollama

```bash
curl -X DELETE http://localhost:2370/links/openwebui-to-ollama
```

**Result:** OpenWebUI can no longer reach Ollama

### Remove External Access

```bash
curl -X DELETE http://localhost:2370/exposures/openwebui
```

**Result:** `openwebui.zeropoint.local` no longer resolves

### Uninstall Apps

```bash
curl -X DELETE http://localhost:2370/apps/openwebui
curl -X DELETE http://localhost:2370/apps/ollama
```

**Result:** All resources cleaned up (containers, networks, images)

---

## Open Questions

1. **Terraform binary distribution**: Bundle with zeropoint container or require host installation?
2. **State backend**: Local `.tfstate` files per module, or centralized remote backend?
3. **Concurrency**: How to handle concurrent `terraform apply` operations on same module?
4. **Module versioning**: Support multiple versions of same app module?
5. **Custom providers**: Build zeropoint-native Terraform provider for advanced features (portals, universes)?
6. **Garbage collection**: Clean up orphaned modules/state when links/exposures are deleted?
7. **Backup strategy**: How to backup/restore all Terraform state across modules?

---

## Summary

zeropoint-apps treats application lifecycle as **declarative infrastructure**:

- Apps are Terraform modules (developers write `.tf` files)
- Links and exposures are generated Terraform modules
- Terraform handles all dependency resolution, state, and execution
- Service discovery via Docker DNS (no host ports)
- External access via single reverse proxy (Caddy)
- zeropoint is a thin orchestrator that generates modules and invokes Terraform

This architecture is simple, correct, and leverages proven tooling. It avoids reinventing resource graphs, state management, or undo logic — Terraform already does this well.
