# zeropoint-agent

<img src="assets/zeropoint-dark.svg" width="400" alt="Zeropoint Logo">

Zeropoint module lifecycle orchestrator that treats containerized modules as declarative infrastructure.

## What is zeropoint-agent?

zeropoint-agent is a REST API server that manages Docker-based modules (ZPMs) using Terraform as the execution engine. Instead of implementing custom resource management, dependency resolution, or state tracking, it delegates all lifecycle operations to Terraform modules while managing higher level operations like network access, storage, variable linking, etc.

**Key Features:**

- **Install, start, stop, and uninstall** modules (containerized services)
- **Link modules** for inter-module communication with variable-linking
- **Expose modules** to the network via Envoy reverse proxy with xDS control plane
- **Zero host port conflicts** - modules use port declarations and service discovery instead of port bindings
- **Terraform-native** - modules are standard Terraform modules using the Docker provider
- **Automatic service discovery** - ports and protocols extracted from module contracts

## Getting Started

### Prerequisites

- [Visual Studio Code](https://code.visualstudio.com/)
- [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
- [Docker](https://docs.docker.com/get-started/get-docker/)

### Development Setup

This project uses VS Code Dev Containers for a consistent development environment.

1. **Clone the repository:**

   ```bash
   git clone https://github.com/zeropoint-os/zeropoint-agent
   cd zeropoint-agent
   ```
2. **Open in VS Code:**

   ```bash
   code .
   ```
3. **Reopen in Container:**

   - Press `F1` or `Cmd/Ctrl+Shift+P`
   - Select: **Dev Containers: Reopen in Container**
   - Wait for the container to build and start
4. **Build/Run the agent:**

   - Use the build task: `Ctrl/Cmd+Shift+B` to build
   - Press `F5` to build & start debugging
   - Or run manually: `go run ./cmd/zeropoint-agent`

The API server will start on `http://localhost:2370` (configurable via `ZEROPOINT_AGENT_PORT` environment variable).

### What's Included in the Dev Container

The dev container provides a complete development environment with:

- **Go 1.25** with gopls, delve debugger
- **Terraform CLI** for zeropoint module management
- **Docker-in-Docker** for managing module containers
- **Development tools**: jq, curl, vim, htop, tree
- **OpenAPI tools**: Swag for generating API documentation

### Install Your First Module

```bash
# Install Ollama
curl -X POST http://localhost:2370/modules/install \
  -H "Content-Type: application/json" \
  -d '{
    "source": "github.com/zeropoint-os/ollama",
    "module_id": "ollama"
  }'

# Check status
curl http://localhost:2370/modules | jq
```

## Architecture

### How It Works

1. **Zeropoint modules are Terraform modules** - Each ZPM is a self-contained `.tf` file that declares Docker resources
2. **Terraform does the heavy lifting** - Dependency graphs, state management, rollback handled by Terraform
3. **REST API orchestrates** - zeropoint-agent invokes `terraform init/apply/destroy` programmatically
4. **Docker DNS for service discovery** - Modules resolve each other by container name (no host ports)
5. **Envoy + xDS for ingress** - External traffic routed via dynamic configuration (no config file reloads)

---

## Architecture

### Module Types

zeropoint manages three types of Terraform modules:

1. **Module Definitions** (`modules/<module-id>/`)

   - Define module resources: image, container, network, volumes
   - Standalone, no dependencies on other modules
   - Written by developers or imported from repositories
2. **Link Modules** (`links/<from-module>-to-<to-module>/`)

   - Connect two modules by attaching one container to another's network
   - Generated by zeropoint when user creates a link
   - Enable service discovery (DNS resolution between modules)
3. **Exposure Modules** (`exposures/<module-id>/`)

   - Configure reverse proxy to route external traffic to a module
   - Generated when user exposes a module via API
   - Attach reverse proxy to module's network
   - zeropoint-agent configures Envoy routes via xDS API (no Terraform module needed)

### Directory Structure

```
zeropoint-agent/
├── modules/                       # Module definitions
│   ├── ollama/
│   │   ├── main.tf
│   │   ├── Dockerfile
│   │   └── terraform.tfstate
│   └── openwebui/
│       ├── main.tf
│       └── terraform.tfstate
├── links/                         # Network link modules (generated)
│   └── openwebui-to-ollama/
│       ├── main.tf
│       └── terraform.tfstate
├── exposures/                     # Reverse proxy exposure tracking (metadata only)
│   └── openwebui/
│       └── metadata.json          # Exposure config (routes managed via xDS)
└── infrastructure/                # Core infrastructure
    └── reverse-proxy/
        ├── main.tf                # Envoy container
        └── terraform.tfstate
```

---

## Module Model

### Example: Ollama Module Definition

**`modules/ollama/main.tf`:**

```hcl
terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 3.0"
    }
  }
}

variable "zp_module_id" {
  type        = string
  description = "Unique identifier for this module instance (auto-generated or user-provided)"
}

variable "zp_network_name" {
  type        = string
  description = "Pre-created Docker network name for this app (managed by zeropoint)"
}

variable "zp_arch" {
  type        = string
  default     = "amd64"
  description = "Target architecture - amd64, arm64, etc. (auto-detected by zeropoint)"
}

variable "zp_gpu_vendor" {
  type        = string
  default     = ""
  description = "GPU vendor - nvidia, amd, intel, or empty for no GPU (auto-detected by zeropoint)"
}

variable "zp_module_storage" {
  type        = string
  description = "Host path for persistent storage (managed by zeropoint)"
}

# Build Ollama image from local Dockerfile
resource "docker_image" "ollama" {
  name = "${var.zp_module_id}:latest"
  build {
    context    = path.module
    dockerfile = "Dockerfile"
    platform   = "linux/${var.zp_arch}"  # Uses auto-detected arch variable
  }
  keep_locally = true
}

# Main Ollama container (no host port binding)
resource "docker_container" "ollama_main" {
  name  = "${var.zp_module_id}-main"
  image = docker_image.ollama.image_id

  # Network configuration (provided by zeropoint)
  networks_advanced {
    name = var.zp_network_name
  }

  # Restart policy
  restart = "unless-stopped"

  # GPU access (conditional based on vendor)
  runtime = var.zp_gpu_vendor == "nvidia" ? "nvidia" : null
  gpus    = var.zp_gpu_vendor != "" ? "all" : null

  # Environment variables
  env = [
    "OLLAMA_HOST=0.0.0.0",
  ]

  # Persistent storage
  volumes {
    host_path      = "${var.zp_module_storage}/.ollama"
    container_path = "/root/.ollama"
  }

  # Ports exposed internally (no host binding)
  # Port 11434 is accessible via service discovery (DNS)
}

# Outputs for zeropoint (container resource only)
output "main" {
  value       = docker_container.ollama_main
  description = "Main Ollama container"
}

# Service ports for external access (defined but not bound to host)
output "main_ports" {
  value = {
    api = {
      port        = 11434                   # Ollama API port
      protocol    = "http"                  # The protocol used
      transport   = "tcp"                   # Transport layer
      description = "Ollama API endpoint"   # Description of the port
      default     = true                    # Default port for the service
    }
  }
  description = "Service ports for external access"
}
```

### Key Characteristics

- **No host ports**: Container exposes ports internally only
- **Isolated network**: Each module has its own Docker bridge network
- **Self-contained**: All dependencies declared in module
- **Output metadata**: Container name, network name for linking

---

## Service Discovery & Communication

### Internal Communication (Module-to-Module)

Modules resolve each other by **container name** within Docker networks:

```
OpenWebUI → Ollama: http://ollama-main:11434
```

Docker's built-in DNS automatically resolves container names to IPs within the same network.

### Network Linking

When OpenWebUI needs to communicate with Ollama:

1. Both modules run in their own isolated networks
2. User creates a **link** via zeropoint API
3. zeropoint generates a link module that attaches OpenWebUI container to Ollama's network
4. DNS resolution works automatically

**No configuration changes to modules required.**

---

## Links: Connecting Modules

### Creating a Link

**API Request:**

```http
POST /links
Content-Type: application/json

{
  "from_module": "openwebui",
  "to_module": "ollama",
  "description": "OpenWebUI needs Ollama API"
}
```

**What zeropoint does:**

1. Generate link module at `links/openwebui-to-ollama/main.tf`:

```hcl
terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 3.0"
    }
  }
}

# Reference existing resources via data sources
data "docker_container" "openwebui" {
  name = "openwebui-main"
}

data "docker_network" "ollama_net" {
  name = "zeropoint-module-ollama"
}

# Create network attachment
resource "docker_network_attachment" "openwebui_to_ollama" {
  container_id = data.docker_container.openwebui.id
  network_id   = data.docker_network.ollama_net.id
}

output "link_established" {
  value = "openwebui can now resolve ollama-main:11434"
}
```

2. Run `terraform init && terraform apply -auto-approve` in link module directory

**Result:**

- OpenWebUI container is now attached to both networks:
  - `zeropoint-module-openwebui` (its own)
  - `zeropoint-module-ollama` (Ollama's)
- OpenWebUI can resolve `ollama-main:11434` via DNS
- No restart or configuration change needed

### Deleting a Link

**API Request:**

```http
DELETE /links/openwebui-to-ollama
```

**What zeropoint does:**

1. Run `terraform destroy -auto-approve` in link module directory
2. Remove `links/openwebui-to-ollama/` directory

**Result:**

- Network attachment is removed
- OpenWebUI can no longer reach Ollama
- Clean, reversible operation

---

## Exposures: External Access via Reverse Proxy

### The Problem

Modules don't bind to host ports. How do users access them from outside?

### The Solution

A central **reverse proxy** (Envoy) routes external traffic to modules by hostname. Exposures are opt-in and explicit. zeropoint-agent acts as an xDS control plane, dynamically configuring Envoy routes without generating Terraform modules.

### Architecture

1. **Infrastructure module** runs Envoy container on host ports 80/443
2. **zeropoint-agent xDS control plane** dynamically configures Envoy listeners/clusters/routes
3. **Metadata-only exposures** stored in `exposures/<module-id>/metadata.json` (no Terraform modules)
4. Envoy resolves module containers by DNS name and proxies traffic
5. Agent uses module contract (`main_ports` output) to discover service ports automatically

### Creating an Exposure

**API Request:**

```http
POST /exposures
Content-Type: application/json

{
  "module_id": "openwebui",
  "hostname": "openwebui.zeropoint.local"
}
```

**What zeropoint-agent does:**

1. Read module contract outputs to discover exposed services:

   ```bash
   terraform output -json main_ports
   # Returns: {"web": {"port": 3000, "protocol": "http", ...}}
   ```
2. Attach Envoy to module's network (one-time Terraform operation):

   ```hcl
   # Terraform module to attach Envoy to module network
   resource "docker_network_attachment" "envoy_to_openwebui" {
     container_id = data.docker_container.envoy.id
     network_id   = data.docker_network.openwebui_net.id
   }
   ```
3. Configure Envoy via xDS API (no Terraform, pure Go):

   ```go
   // Agent pushes new configuration to Envoy via xDS gRPC
   snapshot := cache.NewSnapshot(
     version,
     []types.Resource{}, // endpoints
     []types.Resource{  // clusters
       cluster.NewClusterBuilder().
         WithClusterName("openwebui-web").
         WithDNSLookup("openwebui-main", 3000). // Uses container DNS
         Build(),
     },
     []types.Resource{  // routes
       route.NewRouteConfigurationBuilder().
         WithVirtualHost("openwebui.zeropoint.local", "openwebui-web").
         Build(),
     },
     []types.Resource{  // listeners (already configured for 80/443)
       // Listeners are static, routes are dynamic
     },
   )

   xdsServer.SetSnapshot("openwebui", snapshot)
   ```
4. Store exposure metadata:

   ```json
   // exposures/openwebui/metadata.json
   {
     "module_id": "openwebui",
     "hostname": "openwebui.zeropoint.local",
     "services": {
       "web": {
         "port": 3000,
         "protocol": "http",
         "container_target": "openwebui-main:3000"
       }
     },
     "created_at": "2025-12-25T10:00:00Z"
   }
   ```

**Result:**

- Envoy is attached to `zeropoint-module-openwebui` network (Terraform state tracks this)
- Envoy xDS configuration includes route: `openwebui.zeropoint.local` → `openwebui-main:3000`
- External traffic to `https://openwebui.zeropoint.local` reaches OpenWebUI
- No Terraform destroy needed for route updates (xDS is dynamic)
- Ollama remains unexposed (not accessible externally)

### Deleting an Exposure

**API Request:**

```http
DELETE /exposures/openwebui
```

**What zeropoint-agent does:**

1. Remove route from Envoy via xDS API:

   ```go
   // Update snapshot without the deleted route
   snapshot := cache.NewSnapshot(version+1, ...)
   xdsServer.SetSnapshot("openwebui", snapshot)
   ```
2. Optionally detach Envoy from module network (Terraform destroy):

   ```bash
   terraform destroy -auto-approve
   # Removes docker_network_attachment resource
   ```
3. Delete exposure metadata:

   ```bash
   rm -rf exposures/openwebui/
   ```

**Result:**

- Envoy route removed instantly (xDS push)
- Module network detachment cleaned up by Terraform
- Module is no longer accessible externally

### Benefits of xDS vs Terraform-Generated Config


| Aspect               | xDS Control Plane          | Terraform Config Generation  |
| ---------------------- | ---------------------------- | ------------------------------ |
| **Route updates**    | Instant (gRPC push)        | Slow (terraform apply)       |
| **Config reload**    | Hot reload                 | Container exec reload        |
| **State management** | In-memory + metadata files | Terraform state files        |
| **Scalability**      | Handles 1000s of routes    | Heavy for many small changes |
| **Atomicity**        | Snapshot-based             | Plan/apply lifecycle         |
| **Debugging**        | xDS config dumps           | Parse .tf files              |

**Why Envoy + xDS?**

- **L4/L7 capabilities**: TCP/UDP proxying, HTTP/gRPC routing, WebSocket support
- **Dynamic configuration**: No container restarts, instant route updates
- **Observability**: Built-in metrics, tracing, access logs
- **Future-proof**: Path to service mesh (Istio, Consul Connect)
- **Industry standard**: Same tech powering Kubernetes ingress (Contour, Ambassador)

---

## zeropoint Responsibilities

### What zeropoint DOES

1. **Discover modules**: Scan `modules/` directory for Terraform modules
2. **Invoke Terraform**: Run `terraform init/apply/destroy` programmatically
3. **Generate modules**: Create link modules from API requests (exposures use xDS, not Terraform)
4. **Monitor containers**: Subscribe to Docker events for runtime health tracking
5. **Expose REST API**: Provide HTTP interface for lifecycle operations
6. **Read outputs**: Parse `terraform output -json` to populate API responses and extract service metadata
7. **xDS control plane**: Configure Envoy dynamically via gRPC (listeners, clusters, routes)
8. **Service discovery**: Use `main_ports` output to automatically configure proxy routes without manual port specification

### What zeropoint DOES NOT do

1. ❌ Compute dependency graphs (Terraform does this)
2. ❌ Track resource handles manually (Terraform state does this)
3. ❌ Implement custom destroy logic (Terraform provider handles this)
4. ❌ Manage partial rollback (Terraform state locking handles this)
5. ❌ Allocate host ports (modules use service discovery)
6. ❌ Parse devcontainer.json or docker-compose.yml (developers write Terraform)

---

## API Reference

### System

#### Health Check

```http
GET /health

Response: 200 OK
{
  "status": "healthy"
}

Response: 503 Service Unavailable (Docker unavailable)
{
  "status": "unhealthy",
  "error": "Docker daemon not available"
}
```

### Modules

#### Install Module

```http
POST /modules/{name}
Content-Type: application/json

{
  "source": "https://github.com/zeropoint-os/ollama.git",
  "module_id": "ollama",
  "local_path": "/path/to/module",  // Optional: use local module instead
  "arch": "amd64",                   // Optional: architecture override
  "gpu_vendor": "nvidia"             // Optional: GPU vendor override
}

Response: 200 OK (streaming JSON progress updates)
{
  "status": "downloading",
  "message": "Cloning repository..."
}
```

#### List Modules

```http
GET /modules

Response: 200 OK
{
  "modules": [
    {
      "id": "ollama",
      "state": "running",
      "module_path": "modules/ollama",
      "container_id": "abc123",
      "container_name": "ollama-main",
      "ip_address": "172.20.0.2",
      "containers": {
        "main": {
          "ports": {
            "api": {
              "port": 11434,
              "protocol": "http",
              "transport": "tcp",
              "description": "Ollama API endpoint",
              "default": true
            }
          },
          "mounts": {
            "models": {
              "host_path": "/workspaces/zeropoint-agent/data/modules/ollama/.ollama",
              "container_path": "/root/.ollama",
              "description": "Model storage",
              "read_only": false
            }
          }
        }
      }
    }
  ]
}
```

#### Uninstall Module

```http
DELETE /modules/{name}

Response: 200 OK (streaming JSON progress updates)
{
  "status": "destroying",
  "message": "Removing containers..."
}
```

#### Inspect Module

```http
GET /modules/{module_id}/inspect

Response: 200 OK
{
  "module_id": "ollama",
  "inputs": {
    "zp_module_id": {
      "type": "string",
      "description": "Unique identifier for this module instance",
      "current_value": "ollama",
      "required": true,
      "system_managed": true
    },
    "zp_network_name": {
      "type": "string",
      "description": "Pre-created Docker network name",
      "current_value": "zeropoint-module-ollama",
      "required": true,
      "system_managed": true
    },
    "zp_arch": {
      "type": "string",
      "description": "Target architecture",
      "default_value": "amd64",
      "current_value": "amd64",
      "required": false,
      "system_managed": true
    },
    "zp_gpu_vendor": {
      "type": "string",
      "description": "GPU vendor",
      "default_value": "",
      "current_value": "",
      "required": false,
      "system_managed": true
    },
    "zp_module_storage": {
      "type": "string",
      "description": "Host path for persistent storage",
      "current_value": "/workspaces/zeropoint-agent/data/modules/ollama",
      "required": true,
      "system_managed": true
    }
  },
  "outputs": {
    "main": {
      "description": "Main Ollama container",
      "current_value": { ... }
    },
    "main_ports": {
      "description": "Service ports for external access",
      "current_value": {
        "api": {
          "port": 11434,
          "protocol": "http",
          "transport": "tcp",
          "description": "Ollama API endpoint",
          "default": true
        }
      }
    }
  }
}

# Or inspect from remote source (not yet installed)
GET /modules/{module_id}/inspect?source_url=https://github.com/zeropoint-os/ollama.git

Response: 200 OK
{
  "module_id": "ollama",
  "inputs": { ... },
  "outputs": { ... }
}
```

**Purpose**: Inspect Terraform module to see inputs (variables) and outputs before or after installation. The `system_managed` field indicates which variables are controlled by zeropoint (all `zp_*` variables) vs user-configurable variables.

### Links

#### Create Link

```http
POST /links/{id}
Content-Type: application/json

{
  "modules": {
    "openwebui": {
      "ollama_host": "ollama-main",
      "ollama_port": 11434
    },
    "ollama": {}
  }
}

Response: 200 OK
{
  "success": true,
  "message": "Link created successfully",
  "applied_order": ["ollama", "openwebui"],
  "errors": {}
}
```

#### List Links

```http
GET /links

Response: 200 OK
{
  "links": [
    {
      "id": "openwebui-to-ollama",
      "modules": {
        "openwebui": {
          "ollama_host": "ollama-main",
          "ollama_port": 11434
        },
        "ollama": {}
      },
      "references": {
        "openwebui": {
          "ollama_host": "ollama-main",
          "ollama_port": "11434"
        }
      },
      "shared_networks": ["zeropoint-link-openwebui-to-ollama"],
      "dependency_order": ["ollama", "openwebui"],
      "created_at": "2025-01-02T10:00:00Z",
      "updated_at": "2025-01-02T10:00:00Z"
    }
  ]
}
```

#### Get Link

```http
GET /links/{id}

Response: 200 OK
{
  "id": "openwebui-to-ollama",
  "modules": {
    "openwebui": {
      "ollama_host": "ollama-main",
      "ollama_port": 11434
    },
    "ollama": {}
  },
  "references": {
    "openwebui": {
      "ollama_host": "ollama-main",
      "ollama_port": "11434"
    }
  },
  "shared_networks": ["zeropoint-link-openwebui-to-ollama"],
  "dependency_order": ["ollama", "openwebui"],
  "created_at": "2025-01-02T10:00:00Z",
  "updated_at": "2025-01-02T10:00:00Z"
}

Response: 404 Not Found
{
  "error": "not_found",
  "message": "Link not found"
}
```

#### Delete Link

```http
DELETE /links/{id}

Response: 204 No Content

Response: 404 Not Found
{
  "error": "not_found",
  "message": "Link not found"
}
```

### Exposures

#### Create Exposure

```http
POST /exposures/{exposure_id}
Content-Type: application/json

{
  "module_id": "openwebui",
  "hostname": "openwebui.zeropoint.local",
  "protocol": "http",
  "container_port": 3000
}

Response: 201 Created
{
  "id": "openwebui-web",
  "module_id": "openwebui",
  "hostname": "openwebui.zeropoint.local",
  "protocol": "http",
  "container_port": 3000,
  "host_port": 80,
  "status": "available",
  "created_at": "2025-01-02T10:00:00Z"
}

Response: 200 OK (exposure already exists)
{
  "id": "abc123",
  "module_id": "openwebui",
  "hostname": "openwebui.zeropoint.local",
  "protocol": "http",
  "container_port": 3000,
  "host_port": 80,
  "status": "available",
  "created_at": "2025-01-02T10:00:00Z"
}
```

#### List Exposures

```http
GET /exposures

Response: 200 OK
{
  "exposures": [
    {
      "id": "abc123",
      "module_id": "openwebui",
      "hostname": "openwebui.zeropoint.local",
      "protocol": "http",
      "container_port": 3000,
      "host_port": 80,
      "status": "available",
      "created_at": "2025-01-02T10:00:00Z"
    }
  ]
}
```

#### Get Exposure

```http
GET /exposures/{exposure_id}

Response: 200 OK
{
  "id": "openwebui-web",
  "module_id": "openwebui",
  "hostname": "openwebui.zeropoint.local",
  "protocol": "http",
  "container_port": 3000,
  "host_port": 80,
  "status": "available",
  "created_at": "2025-01-02T10:00:00Z"
}

Response: 404 Not Found
"Exposure not found"
```

#### Delete Exposure

```http
DELETE /exposures/{exposure_id}

Response: 204 No Content

Response: 404 Not Found
"Exposure not found"
```

---

## Data Model

### State Management Architecture

zeropoint uses a **filesystem-first** approach with no persistent database. All state is derived from:

1. **Filesystem structure** (source of truth for what exists)
2. **Terraform state files** (resource metadata and dependencies)
3. **Docker API** (runtime container states)
4. **In-memory cache** (container → module mapping for event routing)

**Discovery on startup:**

```go
// Scan filesystem for modules
func DiscoverModules() ([]Module, error) {
    var modules []Module
    entries, _ := os.ReadDir("modules")
  
    for _, entry := range entries {
        if !entry.IsDir() {
            continue
        }
  
        moduleID := entry.Name()
        modulePath := filepath.Join("modules", moduleID)
  
        // Read Terraform outputs
        outputs := getTerraformOutputs(modulePath)
  
        // Query Docker for actual state
        containers := make(map[string]ContainerInfo)
        for role, container := range outputs {
            state := inspectContainer(container.id)
            containers[role] = ContainerInfo{
                ID:    container.id,
                Name:  container.name,
                State: state,
            }
        }
  
        modules = append(modules, Module{
            ID:         moduleID,
            ModulePath: modulePath,
            Containers: containers,
        })
    }
    return modules
}
```

**Runtime state (in-memory only):**

```go
type RuntimeState struct {
    sync.RWMutex
  
    // Container ID → Module/Role mapping for event routing
    containerRegistry map[string]ModuleContainer
  
    // Module status tracking
    modules map[string]*ModuleState
}

type ModuleContainer struct {
    ModuleID    string
    Role        string  // "main", "db", "worker", etc.
    ContainerID string
    Name        string
}

type ModuleState struct {
    Status          string            // "running", "stopped", "crashed", "degraded"
    LastCheck       time.Time
    ContainerStates map[string]string // role → state
}
```

**Benefits:**

- ✅ No database synchronization issues
- ✅ Filesystem is authoritative source
- ✅ Terraform state tracks all resources
- ✅ Docker API provides runtime state
- ✅ Fast recovery after restart (rebuild from filesystem)
- ✅ Simple disaster recovery (restore filesystem)

---

## Benefits

### Compared to Custom Resource Graph Model


| Aspect                    | Terraform-Based            | Custom Implementation        |
| --------------------------- | ---------------------------- | ------------------------------ |
| **Dependency resolution** | Built-in, proven           | Custom DAG logic needed      |
| **State management**      | Terraform state            | Custom ephemeral store       |
| **Destroy ordering**      | Automatic reverse graph    | Manual undo DAG generation   |
| **Drift detection**       | `terraform plan`           | Not supported                |
| **Idempotency**           | Native                     | Custom reconciliation        |
| **Provider ecosystem**    | Docker + custom providers  | Hand-code each resource type |
| **Visual editor**         | Read state JSON            | Build custom serialization   |
| **Lock-in**               | None (portable`.tf` files) | Locked to custom internals   |
| **Testing**               | Terraform module tests     | Mock entire graph engine     |

### Compared to Host Port Model


| Aspect              | Service Discovery    | Host Port Allocation       |
| --------------------- | ---------------------- | ---------------------------- |
| **Port conflicts**  | Not possible         | Requires tracking registry |
| **App-to-app comm** | Via DNS (simple)     | Via host IP + port         |
| **Security**        | Only proxy exposed   | All apps exposed to host   |
| **Scaling**         | Easy (auto-register) | Hard (port conflicts)      |
| **Configuration**   | Network attachments  | Port mappings in state     |
| **External access** | Single reverse proxy | Direct per-app             |

---

## Module Contract

### Overview

zeropoint enforces a minimum contract for modules to ensure predictable behavior and interoperability. Before applying a module, zeropoint performs a **dry-run validation** (`terraform plan`) to verify compliance. Non-compliant modules are rejected with clear error messages.

### Required Components

#### 1. Required Variables

Every module MUST accept the following system-managed variables (all prefixed with `zp_`):

```hcl
variable "zp_module_id" {
  type        = string
  description = "Unique identifier for this app instance (provided via API path parameter)"
}

variable "zp_network_name" {
  type        = string
  description = "Pre-created Docker network name for this app (managed by zeropoint)"
}

variable "zp_arch" {
  type        = string
  default     = "amd64"
  description = "Target architecture (amd64, arm64, etc.) - auto-detected by zeropoint"
}

variable "zp_gpu_vendor" {
  type        = string
  default     = ""
  description = "GPU vendor (nvidia, amd, intel, or empty) - auto-detected by zeropoint"
}

variable "zp_module_storage" {
  type        = string
  description = "Absolute path to app's persistent storage directory - managed by zeropoint"
}
```

**Purpose**:

- `zp_module_id`: Unique identifier from API request path (e.g., `POST /modules/production-ollama` → `zp_module_id = "production-ollama"`). Enables multi-instance deployments.
- `zp_network_name`: zeropoint creates and manages the network lifecycle externally, then injects it into the module
- `zp_arch`: Target CPU architecture, auto-detected by zeropoint from host (user-overridable via API). Module authors can use this for `platform` selection in builds or ignore it for multi-arch images
- `zp_gpu_vendor`: GPU vendor detected by zeropoint (`nvidia`, `amd`, `intel`, or empty string). Apps can conditionally enable GPU access based on this value
- `zp_module_storage`: Absolute path to the module's isolated storage directory (e.g., `/workspaces/zeropoint-agent/data/modules/production-ollama`). Modules use this for persistent data like databases, models, configuration, etc.

**Variable Contract**:
- All `zp_*` variables are **system-managed** - injected by zeropoint, not user-editable
- Apps can define additional user-configurable variables (e.g., `model_size`, `db_password`)
- The `system_managed` field in the inspect API distinguishes between zeropoint variables and user variables

**App Storage Usage**:

Apps organize their persistent data under `zp_module_storage` however they want:

```hcl
# Example: Ollama stores models
volumes {
  host_path      = "${var.zp_module_storage}/.ollama"
  container_path = "/root/.ollama"
}

# Example: Database app with multiple volumes
volumes {
  host_path      = "${var.zp_module_storage}/data"
  container_path = "/var/lib/postgresql/data"
}

volumes {
  host_path      = "${var.zp_module_storage}/config"
  container_path = "/etc/postgresql"
}
```

**Storage guarantees**:
- zeropoint creates `zp_module_storage` directory before terraform apply
- Path is always absolute (required by Docker bind mounts)
- Directory is unique per module instance (isolated by `zp_module_id`)
- Storage persists across container restarts
- Storage is deleted when app is uninstalled

**Rationale**: zeropoint creates the network **before** applying the module. This:

- Separates network lifecycle from container lifecycle
- Makes link module generation simpler (zeropoint already knows the network ID)
- Reduces boilerplate in modules
- Enables network to exist independently of containers

**Validation**: Module must declare both `module_id` and `network_name` variables. zeropoint will inject these during installation.

#### 2. Required Resources

Every module MUST declare at minimum:

- **One `docker_container` resource named `${var.module_id}_main`**: The primary application container
- **Additional containers** (optional): Sidecars, databases, workers, etc.

**Note**: Networks are **NOT** declared in modules. zeropoint creates the network externally and passes it via `var.network_name`.

**Single-Container Example (module_id="ollama"):**

```hcl
resource "docker_container" "ollama_main" {
  name  = "${var.module_id}-main"
  image = docker_image.app.image_id
  
  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }
}
```

**Multi-Container Example (module_id="myapp"):**

```hcl
# Main application container (REQUIRED)
resource "docker_container" "myapp_main" {
  name  = "${var.module_id}-main"
  image = docker_image.app.image_id
  
  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }
  
  env = [
    "DB_HOST=${docker_container.myapp_db.name}:5432",
  ]
}

# Database sidecar (OPTIONAL)
resource "docker_container" "myapp_db" {
  name  = "${var.module_id}-db"
  image = "postgres:15"
  
  networks_advanced {
    name = var.network_name  # Same network as main container
  }
}

# Worker sidecar (OPTIONAL)
resource "docker_container" "myapp_worker" {
  name  = "${var.module_id}-worker"
  image = docker_image.app.image_id
  
  networks_advanced {
    name = var.network_name  # Same network as main container
  }
  
  command = ["worker", "--queue=jobs"]
}
```

**Key Points:**

- The `${var.module_id}_main` container is the **primary entry point** for service discovery
- Terraform resource names MUST use pattern: `${var.module_id}_<role>` (e.g., `ollama_main`, `ollama_db`)
- Runtime container names use hyphens: `${var.module_id}-<role>` (e.g., `ollama-main`, `ollama-db`)
- Additional containers MUST use `var.network_name` (same network for internal communication)
- Additional containers SHOULD follow role naming: `_db`, `_worker`, `_redis`, etc.
- All containers within an app communicate via container names on the shared network
- Network is created by zeropoint **before** applying the module

**Multi-Instance Support:**

Users can deploy multiple instances of the same module by providing unique `module_id` values:

- Production: `module_id="ollama"` → resources: `ollama_main`, `ollama_db`
- Testing: `module_id="ollama-test"` → resources: `ollama-test_main`, `ollama-test_db`
- Alice's instance: `module_id="ollama-alice"` → resources: `ollama-alice_main`, `ollama-alice_db`
- Project A: `module_id="projecta-db"` → resources: `projecta-db_main`

**Note**: `module_id` is a freeform unique identifier. Users choose meaningful names for their use case.

**Validation**: `terraform plan` must show at least one `docker_container` resource with Terraform resource name exactly matching `${module_id}_main`.

#### 3. Required Outputs

Every module MUST expose outputs for containers and service ports.

##### Container Outputs (Required)

All container outputs MUST be `docker_container` resource references, named by role.

**Single-Container Example (module_id="ollama"):**

```hcl
output "main" {
  value       = docker_container.ollama_main
  description = "Main application container"
}
```

**Multi-Container Example (module_id="myapp"):**

```hcl
output "main" {
  value       = docker_container.myapp_main
  description = "Main application container"
}

output "db" {
  value       = docker_container.myapp_db
  description = "Database sidecar container"
}

output "worker" {
  value       = docker_container.myapp_worker
  description = "Background worker container"
}
```

**Contract Rules**:

- Output name = container role (`main`, `db`, `worker`, `cache`, etc.)
- Output value = `docker_container.<resource_name>` (entire resource object)
- `main` output is **REQUIRED** (primary application container)
- Additional outputs are optional (sidecars, workers, etc.)
- All outputs MUST be `docker_container` resources (no primitives, no other resource types)

##### Service Port Metadata (Required for Exposures)

Apps MUST declare a `main_ports` output that describes which ports provide externally-accessible services. This metadata enables zeropoint-agent to automatically configure Envoy routes without requiring users to manually specify ports.

**Structure:**

```hcl
output "main_ports" {
  value = {
    <service_name> = {
      port        = <number>           # Container port number
      protocol    = "<http|grpc|tcp>"  # Application protocol
      transport   = "<tcp|udp>"        # Transport protocol (default: tcp)
      description = "<string>"         # Human-readable description
      default     = <bool>             # Is this the primary service? (optional, one per app)
    }
  }
  description = "Externally-accessible service ports for proxy configuration"
}
```

**Single-Service Example (Ollama API):**

```hcl
output "main_ports" {
  value = {
    api = {
      port        = 11434
      protocol    = "http"
      transport   = "tcp"
      description = "Ollama API endpoint"
      default     = true
    }
  }
}
```

**Multi-Service Example (App with Web UI and API):**

```hcl
output "main_ports" {
  value = {
    web = {
      port        = 3000
      protocol    = "http"
      transport   = "tcp"
      description = "Web UI"
      default     = true  # Primary service
    }
    api = {
      port        = 8080
      protocol    = "grpc"
      transport   = "tcp"
      description = "gRPC API"
    }
    metrics = {
      port        = 9090
      protocol    = "http"
      transport   = "tcp"
      description = "Prometheus metrics endpoint"
    }
  }
}
```

**TCP Service Example (Database):**

```hcl
output "main_ports" {
  value = {
    postgres = {
      port        = 5432
      protocol    = "tcp"
      transport   = "tcp"
      description = "PostgreSQL wire protocol"
      default     = true
    }
  }
}
```

**How zeropoint-agent uses `main_ports`:**

1. **Automatic exposure**: When exposing an app, agent reads `main_ports` to discover services
2. **Default service selection**: If user doesn't specify a service, use `default = true` entry
3. **Protocol-aware routing**:

   - `http`: HTTP/1.1 or HTTP/2 routing with Host header matching
   - `grpc`: HTTP/2 routing with gRPC protocol negotiation
   - `tcp`: L4 TCP proxy (no HTTP parsing)
4. **Multi-service support**: Users can expose individual services from same app:

   ```bash
   POST /exposures {"module_id": "myapp", "service": "web", "hostname": "app.example.com"}
   POST /exposures {"module_id": "myapp", "service": "api", "hostname": "api.example.com"}
   ```
5. **Info endpoints**: Agent can expose metadata about available services:

   ```bash
   GET /modules/mymodule/services
   # Returns: {"web": {"port": 3000, "protocol": "http", ...}, ...}
   ```

**Validation Rules:**

- MUST be a map with string keys (service names)
- Each service MUST have `port` (number), `protocol` (string), `description` (string)
- `transport` defaults to `"tcp"` if omitted
- `protocol` MUST be one of: `http`, `grpc`, `tcp`
- At most one service can have `default = true`
- Service names MUST be valid DNS labels (alphanumeric + hyphens, start with letter)

**What zeropoint extracts from outputs:**

```go
// For each container output, zeropoint reads:
containers[role] = ContainerInfo{
    ID:    output.id,           // Container ID for Docker event monitoring
    Name:  output.name,         // Container name for DNS resolution
}

// From main_ports output:
services := make(map[string]ServiceInfo)
for serviceName, svcConfig := range mainPorts {
    services[serviceName] = ServiceInfo{
        Port:        svcConfig.Port,
        Protocol:    svcConfig.Protocol,
        Transport:   svcConfig.Transport,
        Description: svcConfig.Description,
        IsDefault:   svcConfig.Default,
    }
}
```

**Rationale**:

- **No manual port specification**: Users don't need to remember port numbers when exposing apps
- **Self-documenting**: App contract includes service metadata (names, descriptions, protocols)
- **Multi-service support**: Single app can expose multiple endpoints (web UI, API, metrics, admin)
- **Protocol-aware routing**: Envoy can use correct routing logic (HTTP headers vs TCP streams)
- **Info endpoints**: Agent can provide service discovery information without querying containers
- **Convention over configuration**: Developers declare ports once, agent uses them everywhere

#### 4. Naming Conventions

Modules MUST follow these naming patterns:


| Resource                            | Pattern                             | Example                        | Notes                                  |
| ------------------------------------- | ------------------------------------- | -------------------------------- | ---------------------------------------- |
| Main container (Terraform resource) | `${var.module_id}_main`                | `docker_container.ollama_main` | Exact match required                   |
| Main container (runtime name)       | `${var.module_id}-main`                | `ollama-main`                  | DNS-resolvable name (hyphen)           |
| Additional containers (Terraform)   | `${var.module_id}_<role>`              | `docker_container.ollama_db`   | Underscores in resource names          |
| Additional containers (runtime)     | `${var.module_id}-<role>`              | `ollama-db`, `ollama-worker`   | Hyphens in runtime names               |
| Network                             | `zeropoint-module-${var.module_id}`       | `zeropoint-module-ollama`         | Created by zeropoint, passed to module |
| Images                              | `zeropoint-module-${var.module_id}:<tag>` | `zeropoint-module-ollama:latest`  | Optional, can use registry images      |

**Purpose**: Predictable resource discovery for link/exposure generation.

**Multi-Container Naming Example (module_id="myapp"):****

```hcl
resource "docker_container" "myapp_main" {
  name = "${var.module_id}-main"  # Runtime: "myapp-main"
  
  networks_advanced {
    name = var.network_name  # "zeropoint-module-myapp" (created by zeropoint)
  }
}

resource "docker_container" "myapp_db" {
  name = "${var.module_id}-db"    # Runtime: "myapp-db"
  
  networks_advanced {
    name = var.network_name
  }
}

resource "docker_container" "myapp_worker" {
  name = "${var.module_id}-worker" # Runtime: "myapp-worker"
  
  networks_advanced {
    name = var.network_name
  }
}
```

**Multi-Instance Example (module_id="myapp-test"):**

```hcl
resource "docker_container" "myapp-test_main" {
  name = "${var.module_id}-main"  # Runtime: "myapp-test-main"
  
  networks_advanced {
    name = var.network_name  # "zeropoint-module-myapp-test"
  }
}

resource "docker_container" "myapp-test_db" {
  name = "${var.module_id}-db"    # Runtime: "myapp-test-db"
  
  networks_advanced {
    name = var.network_name
  }
}
```

**Note**: Network (`zeropoint-module-${var.module_id}`) is created by zeropoint **before** module is applied.

**Validation**: zeropoint inspects `terraform plan` JSON output to verify:

- Exactly one container resource with Terraform name `${module_id}_main` (exact match)
- Main container runtime name matches `${module_id}-main` (hyphenated)
- All containers reference `var.network_name` (not a local resource)
- No `docker_network` resources declared in module

#### 5. No Host Port Bindings (Container Resources)

Modules MUST NOT include `ports` blocks in `docker_container` resources:

❌ **Invalid:**

```hcl
resource "docker_container" "main" {
  name  = "${var.module_id}-main"
  image = docker_image.app.image_id
  
  ports {
    internal = 8080
    external = 30001  # ❌ Not allowed
  }
}
```

✅ **Valid:**

```hcl
resource "docker_container" "main" {
  name  = "${var.module_id}-main"
  image = docker_image.app.image_id
  
  # Ports exposed internally via Dockerfile EXPOSE, no host binding
}
```

**Rationale**: Service discovery via DNS replaces host ports. External access is managed by exposure modules.

**Validation**: zeropoint checks `terraform plan` output for `ports` configuration in container resources.

### Validation Process

#### Dry-Run Validation

When a user installs a module via `POST /modules/{name}`:

```http
POST /modules/mycustomapp
{
  "source": "https://github.com/user/mycustomapp.git"
}
```

**zeropoint performs these checks:**

1. **Terraform Init**

   ```bash
   cd /modules/mycustomapp
   terraform init
   ```

   **Success criteria**: Exit code 0, providers downloaded
2. **Terraform Plan (Dry-Run)**

   ```bash
   terraform plan -var="module_id=mycustomapp" -out=/tmp/plan.tfplan
   ```

   **Success criteria**: Exit code 0, plan file generated
3. **Extract Plan JSON**

   ```bash
   terraform show -json /tmp/plan.tfplan > /tmp/plan.json
   ```
4. **Validate Required Resources**

   - Parse `plan.json`
   - Verify `resource_changes` includes:
     - At least one `docker_container` resource with Terraform resource name ending in `_main`
   - Verify module does **not** create `docker_network` resources (zeropoint manages networks)
   - Additional containers (sidecars, etc.) are permitted but not required
5. **Validate Naming Conventions**

   - Check main `docker_container.name` == `"${module_id}-main"`
   - Verify containers use `var.network_name` (not a locally-created network)
6. **Validate No Host Ports**

   - Check `docker_container.ports` is empty or absent
   - Reject if any `external` port bindings exist
7. **Validate Required Outputs**

   - Run `terraform output -json` on plan
   - Verify at least one output named `"main"` exists
   - Verify `"main"` output is a `docker_container` resource
   - Verify all container outputs are `docker_container` resources (reject non-container outputs)
   - Verify `"main_ports"` output exists and is a valid service port map
   - Validate `main_ports` structure:
     - Each service has required fields: `port`, `protocol`, `description`
     - `protocol` is one of: `http`, `grpc`, `tcp`
     - `transport` (if present) is one of: `tcp`, `udp`
     - At most one service has `default = true`
     - Service names are valid DNS labels

**If all checks pass**: Proceed with `terraform apply -auto-approve`

**If any check fails**: Return HTTP 400 with detailed error:

```json
{
  "error": "Module validation failed",
  "details": [
    "Missing required output: 'main' (must reference docker_container resource)",
    "Missing required output: 'main_ports' (must be a map of service port definitions)",
    "Service 'api' in main_ports missing required field: 'protocol'",
    "Service 'web' in main_ports has invalid protocol: 'websocket' (must be http, grpc, or tcp)",
    "Multiple services have default=true (only one allowed): web, api",
    "Output 'config' is not a docker_container resource (non-container outputs not allowed)",
    "Container name 'mycustomapp-app' does not match required pattern '${module_id}-main'",
    "Host port binding detected on container 'mycustomapp-app' (port 8080:30001)"
  ]
}
```

### Validation Implementation (Go)

```go
package validator

import (
    "encoding/json"
    "fmt"
    "os/exec"
    "path/filepath"
)

type PlanOutput struct {
    ResourceChanges []ResourceChange `json:"resource_changes"`
}

type ResourceChange struct {
    Type    string                 `json:"type"`
    Name    string                 `json:"name"`
    Change  Change                 `json:"change"`
}

type Change struct {
    After map[string]interface{} `json:"after"`
}

func ValidateAppModule(modulePath, appID string) error {
    // Step 1: Terraform init
    if err := runTerraform(modulePath, "init"); err != nil {
        return fmt.Errorf("terraform init failed: %w", err)
    }

    // Step 2: Terraform plan
    planFile := filepath.Join("/tmp", fmt.Sprintf("%s-plan.tfplan", appID))
    if err := runTerraformPlan(modulePath, appID, planFile); err != nil {
        return fmt.Errorf("terraform plan failed: %w", err)
    }

    // Step 3: Extract plan JSON
    planJSON, err := extractPlanJSON(planFile)
    if err != nil {
        return fmt.Errorf("failed to extract plan JSON: %w", err)
    }

    // Step 4-6: Validate resources and naming
    if err := validateResources(planJSON, appID); err != nil {
        return err
    }

    // Step 7: Validate outputs
    if err := validateOutputs(modulePath); err != nil {
        return err
    }

    return nil
}

func validateResources(planJSON []byte, appID string) error {
    var plan PlanOutput
    if err := json.Unmarshal(planJSON, &plan); err != nil {
        return err
    }

    var foundMainContainer bool
    var errors []string

    expectedResourceName := fmt.Sprintf("%s_main", appID)
    expectedContainerName := fmt.Sprintf("%s-main", appID)

    for _, rc := range plan.ResourceChanges {
        switch rc.Type {
        case "docker_container":
            // Check if this is the main container (resource name must be exactly "${module_id}_main")
            isMainContainer := rc.Name == expectedResourceName
      
            if isMainContainer {
                foundMainContainer = true
          
                // Check runtime naming for main container
                if name, ok := rc.Change.After["name"].(string); ok {
                    if name != expectedContainerName {
                        errors = append(errors, 
                            fmt.Sprintf("Main container runtime name '%s' does not match required pattern '%s'", 
                                name, expectedContainerName))
                    }
                }
            }

            // Check for host port bindings on ANY container
            if ports, ok := rc.Change.After["ports"].([]interface{}); ok && len(ports) > 0 {
                containerName := rc.Change.After["name"].(string)
                errors = append(errors, 
                    fmt.Sprintf("Host port bindings detected on container '%s' (not allowed)", containerName))
            }

        case "docker_network":
            // Modules should NOT create networks (zeropoint manages them)
            errors = append(errors, 
                fmt.Sprintf("Module must not create docker_network resources (network is provided via var.network_name)"))
        }
    }

    if !foundMainContainer {
        errors = append(errors, fmt.Sprintf("Missing required resource: docker_container.%s (Terraform resource name must be exactly '${module_id}_main')", expectedResourceName))
    }

    if len(errors) > 0 {
        return fmt.Errorf("validation errors:\n- %s", strings.Join(errors, "\n- "))
    }

    return nil
}

func validateOutputs(modulePath string) error {
    cmd := exec.Command("terraform", "output", "-json")
    cmd.Dir = modulePath
  
    output, err := cmd.Output()
    if err != nil {
        return fmt.Errorf("failed to read outputs: %w", err)
    }

    var outputs map[string]interface{}
    if err := json.Unmarshal(output, &outputs); err != nil {
        return err
    }

    var errors []string

    // Check for required 'main' container output
    mainOutput, hasMain := outputs["main"]
    if !hasMain {
        errors = append(errors, "missing required output: 'main' (must reference docker_container resource)")
    } else if !isContainerResource(mainOutput) {
        errors = append(errors, "output 'main' is not a docker_container resource")
    }

    // Check for required 'main_ports' output
    mainPortsOutput, hasMainPorts := outputs["main_ports"]
    if !hasMainPorts {
        errors = append(errors, "missing required output: 'main_ports' (must be a map of service port definitions)")
    } else {
        // Validate main_ports structure
        if portErrors := validateMainPorts(mainPortsOutput); len(portErrors) > 0 {
            errors = append(errors, portErrors...)
        }
    }

    // Verify all non-main_ports outputs are container resources
    for name, val := range outputs {
        if name == "main_ports" {
            continue // Skip main_ports (it's metadata, not a container)
        }
        if !isContainerResource(val) {
            errors = append(errors, 
                fmt.Sprintf("output '%s' is not a docker_container resource (non-container outputs not allowed)", name))
        }
    }

    if len(errors) > 0 {
        return fmt.Errorf("validation errors:\n- %s", strings.Join(errors, "\n- "))
    }

    return nil
}

func validateMainPorts(output interface{}) []string {
    var errors []string
  
    outputMap, ok := output.(map[string]interface{})
    if !ok {
        return []string{"main_ports is not a map"}
    }
  
    value, ok := outputMap["value"].(map[string]interface{})
    if !ok {
        return []string{"main_ports.value is not a map"}
    }
  
    validProtocols := map[string]bool{"http": true, "grpc": true, "tcp": true}
    validTransports := map[string]bool{"tcp": true, "udp": true}
    defaultCount := 0
  
    for serviceName, svcConfig := range value {
        svcMap, ok := svcConfig.(map[string]interface{})
        if !ok {
            errors = append(errors, fmt.Sprintf("service '%s' is not a valid configuration map", serviceName))
            continue
        }
  
        // Validate required fields
        if _, hasPort := svcMap["port"]; !hasPort {
            errors = append(errors, fmt.Sprintf("service '%s' missing required field: 'port'", serviceName))
        }
  
        protocol, hasProtocol := svcMap["protocol"].(string)
        if !hasProtocol {
            errors = append(errors, fmt.Sprintf("service '%s' missing required field: 'protocol'", serviceName))
        } else if !validProtocols[protocol] {
            errors = append(errors, 
                fmt.Sprintf("service '%s' has invalid protocol: '%s' (must be http, grpc, or tcp)", 
                    serviceName, protocol))
        }
  
        if _, hasDesc := svcMap["description"]; !hasDesc {
            errors = append(errors, fmt.Sprintf("service '%s' missing required field: 'description'", serviceName))
        }
  
        // Validate optional transport field
        if transport, hasTransport := svcMap["transport"].(string); hasTransport {
            if !validTransports[transport] {
                errors = append(errors, 
                    fmt.Sprintf("service '%s' has invalid transport: '%s' (must be tcp or udp)", 
                        serviceName, transport))
            }
        }
  
        // Track default services
        if isDefault, ok := svcMap["default"].(bool); ok && isDefault {
            defaultCount++
        }
  
        // Validate service name is a valid DNS label
        if !isValidDNSLabel(serviceName) {
            errors = append(errors, 
                fmt.Sprintf("service name '%s' is not a valid DNS label (alphanumeric + hyphens, start with letter)", 
                    serviceName))
        }
    }
  
    if defaultCount > 1 {
        errors = append(errors, "multiple services have default=true (only one allowed)")
    }
  
    return errors
}

func isValidDNSLabel(s string) bool {
    if len(s) == 0 || len(s) > 63 {
        return false
    }
    if !unicode.IsLetter(rune(s[0])) {
        return false
    }
    for _, ch := range s {
        if !unicode.IsLetter(ch) && !unicode.IsDigit(ch) && ch != '-' {
            return false
        }
    }
    return true
}

func isContainerResource(output interface{}) bool {
    outputMap, ok := output.(map[string]interface{})
    if !ok {
        return false
    }
  
    // Check if output has container-specific attributes
    value, ok := outputMap["value"].(map[string]interface{})
    if !ok {
        return false
    }
  
    // Container resources have 'id', 'name', and 'image' attributes
    _, hasID := value["id"]
    _, hasName := value["name"]
    _, hasImage := value["image"]
  
    return hasID && hasName && hasImage
}
```

### Optional Components

Modules MAY include:

- **Additional containers**: Sidecars, init containers, databases, workers (follow `${module_id}-<role>` naming)
- **Volumes**: For persistent data (`docker_volume` resources)
- **Images from registry**: Pull from Docker Hub instead of building locally
- **Environment variables**: Via `env` blocks in containers
- **Health checks**: Via `healthcheck` blocks
- **Custom variables**: Beyond `module_id`, for user configuration
- **Dependencies between containers**: Use `depends_on` to ensure ordering

**Example of optional components (multi-container app with database):**

```hcl
variable "model_size" {
  type        = string
  default     = "7b"
  description = "Ollama model size to download"
}

# Note: arch variable is already injected by zeropoint, can be used for platform-specific builds
# For multi-arch images pulled from registries, you can ignore var.arch

variable "db_password" {
  type        = string
  sensitive   = true
  description = "PostgreSQL password"
}

# Persistent volume for database
resource "docker_volume" "db_data" {
  name = "zeropoint-module-${var.module_id}-db"
}

# Persistent volume for app data
resource "docker_volume" "app_data" {
  name = "zeropoint-module-${var.module_id}-data"
}

# Database sidecar (starts first)
resource "docker_container" "myapp_db" {
  name  = "${var.module_id}-db"
  image = "postgres:15"

  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }

  volumes {
    volume_name    = docker_volume.db_data.name
    container_path = "/var/lib/postgresql/data"
  }

  env = [
    "POSTGRES_PASSWORD=${var.db_password}",
    "POSTGRES_DB=appdb",
  ]

  healthcheck {
    test     = ["CMD-SHELL", "pg_isready -U postgres"]
    interval = "10s"
    timeout  = "5s"
    retries  = 5
  }
}

# Main application container (waits for db)
resource "docker_container" "myapp_main" {
  name  = "${var.module_id}-main"
  image = docker_image.app.image_id

  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }

  volumes {
    volume_name    = docker_volume.app_data.name
    container_path = "/data"
  }

  env = [
    "DB_HOST=${docker_container.myapp_db.name}",
    "DB_PORT=5432",
    "DB_NAME=appdb",
    "MODEL_SIZE=${var.model_size}",
  ]

  depends_on = [docker_container.myapp_db]  # Wait for database

  healthcheck {
    test     = ["CMD", "curl", "-f", "http://localhost:8080/health"]
    interval = "30s"
    timeout  = "10s"
    retries  = 3
  }
}

# Background worker (optional)
resource "docker_container" "myapp_worker" {
  name  = "${var.module_id}-worker"
  image = docker_image.app.image_id

  networks_advanced {
    name = var.network_name  # Provided by zeropoint
  }

  command = ["worker", "--queue=jobs"]

  env = [
    "DB_HOST=${docker_container.myapp_db.name}",
    "DB_PORT=5432",
  ]

  depends_on = [docker_container.myapp_db, docker_container.myapp_main]
}

# Required outputs (all containers as resources)
output "main" {
  value = docker_container.myapp_main
}

output "db" {
  value = docker_container.myapp_db
}

output "worker" {
  value = docker_container.myapp_worker
}
```

**Key Notes for Multi-Container Apps:**

- All containers share the same network (provided via `var.network_name`, created by zeropoint)
- Containers communicate via DNS (e.g., main container reaches db at `${module_id}-db:5432`)
- Use `depends_on` to ensure containers start in the correct order
- Outputs always reference the **main** container (the primary service endpoint)
- Links attach external apps to the shared network, giving them access to all containers

### Non-Compliant Module Examples

#### ❌ Missing Required Output

```hcl
# Missing: output "main"
output "database" {
  value = docker_container.myapp_db
}

output "worker" {
  value = docker_container.myapp_worker
}
```

**Error:**

```
Module validation failed: Missing required output: 'main' (must reference docker_container resource)
```

#### ❌ Non-Container Output

```hcl
output "main" {
  value = docker_container.myapp_main
}

# ❌ Not allowed - outputs must be container resources only
output "admin_password" {
  value = random_password.admin.result
}
```

**Error:**

```
Module validation failed: output 'admin_password' is not a docker_container resource (non-container outputs not allowed)
```

#### ❌ Wrong Resource or Container Naming

```hcl
# Wrong Terraform resource name
resource "docker_container" "main" {  # ❌ Should be "${var.module_id}_main" e.g., "myapp_main"
  name  = "${var.module_id}-main"
  image = "myapp:latest"
}

# Or wrong runtime container name
resource "docker_container" "myapp_main" {
  name  = "myapp-container"  # ❌ Should be "${var.module_id}-main"
  image = "myapp:latest"
}
```

**Errors:**

```
Missing required resource: docker_container.myapp_main (Terraform resource name must be exactly '${module_id}_main')
```

or

```
Main container runtime name 'myapp-container' does not match required pattern 'myapp-main'
```

#### ❌ Host Port Binding

```hcl
resource "docker_container" "myapp_main" {
  name  = "${var.module_id}-main"
  image = "myapp:latest"

  ports {
    internal = 8080
    external = 30001  # ❌ Not allowed
  }
}
```

**Error:**

```
Host port bindings detected on container 'main' (not allowed)
```

#### ❌ Module Creates Network

```hcl
# ❌ Modules must NOT create networks
resource "docker_network" "app_net" {
  name = "zeropoint-module-${var.module_id}"
}

resource "docker_container" "myapp_main" {
  name  = "${var.module_id}-main"
  image = "myapp:latest"
  
  networks_advanced {
    name = docker_network.app_net.name  # Should use var.network_name instead
  }
}
```

**Error:**

```
Module must not create docker_network resources (network is provided via var.network_name)
```

### Contract Enforcement Benefits

1. **Predictable Link Generation**: Container/network names follow known patterns
2. **Service Discovery Works**: Container names guaranteed to be DNS-compatible
3. **No Port Conflicts**: Host port allocation forbidden at app level
4. **API Consistency**: All apps return same output structure
5. **Developer Clarity**: Contract documented, violations clearly reported
6. **Future-Proof**: Can extend contract without breaking existing modules

---

## Implementation Roadmap

### Phase 1: Core Terraform Integration

- [X] Define app module structure
- [X] Test Terraform apply/destroy workflow
- [ ] Implement app module contract validation
- [ ] Implement `hashicorp/terraform-exec` Go SDK integration
- [ ] Add Terraform binary to deployment image
- [ ] Implement basic module lifecycle API (`/modules/install`, `/modules/{id}/start`, etc.)

### Phase 2: Links

- [ ] Design link module template
- [ ] Implement `POST /links` (generate module, apply)
- [ ] Implement `DELETE /links/{id}` (destroy module)
- [ ] Implement filesystem-based link discovery
- [ ] Test multi-container app communication

### Phase 3: Exposures & xDS Control Plane

- [ ] Deploy Envoy infrastructure module
- [ ] Implement xDS control plane using `github.com/envoyproxy/go-control-plane`
- [ ] Design snapshot-based configuration (clusters, routes, listeners)
- [ ] Implement `POST /exposures` (attach Envoy to network, push xDS config)
- [ ] Implement `DELETE /exposures/{id}` (remove route from xDS, detach network)
- [ ] Add service discovery from `main_ports` output
- [ ] Implement metadata-only exposure tracking (no Terraform modules)
- [ ] Support multi-service exposures (single app, multiple hostnames)

### Phase 4: Developer Experience

- [ ] Document app module contract (required outputs, naming conventions)
- [ ] Provide example modules (`modules/examples/`)
- [ ] Add module validation (`terraform validate`)
- [ ] Implement dry-run mode (`terraform plan` without apply)
- [ ] Add graph visualization endpoint (`GET /modules/{id}/graph`)

### Phase 5: Advanced Features

- [ ] Custom Terraform providers for zeropoint-native resources
- [ ] Multi-module composition (parent modules)
- [ ] Backup/restore of Terraform state
- [ ] Remote state backend support (S3, Consul)
- [ ] Web UI for visual graph editing

---

## Example Workflow

### Install Ollama

```bash
curl -X POST http://localhost:2370/modules/install \
  -H "Content-Type: application/json" \
  -d '{
    "module_path": "/modules/ollama",
    "module_id": "ollama"
  }'
```

### Install OpenWebUI

```bash
curl -X POST http://localhost:2370/modules/install \
  -H "Content-Type: application/json" \
  -d '{
    "module_path": "/modules/openwebui",
    "module_id": "openwebui"
  }'
```

### Link OpenWebUI to Ollama

```bash
curl -X POST http://localhost:2370/links \
  -H "Content-Type: application/json" \
  -d '{
    "from_app": "openwebui",
    "to_app": "ollama"
  }'
```

**Result:** OpenWebUI can now call `http://ollama-main:11434`

### Expose OpenWebUI to the World

```bash
curl -X POST http://localhost:2370/exposures \
  -H "Content-Type: application/json" \
  -d '{
    "module_id": "openwebui",
    "hostname": "openwebui.zeropoint.local",
    "container_port": 3000
  }'
```

**Result:** Visit `https://openwebui.zeropoint.local` in browser

### Unlink OpenWebUI from Ollama

```bash
curl -X DELETE http://localhost:2370/links/openwebui-to-ollama
```

**Result:** OpenWebUI can no longer reach Ollama

### Remove External Access

```bash
curl -X DELETE http://localhost:2370/exposures/openwebui
```

**Result:** `openwebui.zeropoint.local` no longer resolves

### Uninstall Modules

```bash
curl -X DELETE http://localhost:2370/modules/openwebui
curl -X DELETE http://localhost:2370/modules/ollama
```

**Result:** All resources cleaned up (containers, networks, images)

---

## Open Questions

1. **Terraform binary distribution**: Bundle with zeropoint container or require host installation?
2. **State backend**: Local `.tfstate` files per module, or centralized remote backend?
3. **Concurrency**: How to handle concurrent `terraform apply` operations on same module?
4. **Module versioning**: Support multiple versions of same module?
5. **Custom providers**: Build zeropoint-native Terraform provider for advanced features (portals, universes)?
6. **Garbage collection**: Clean up orphaned modules/state when links/exposures are deleted?
7. **Backup strategy**: How to backup/restore all Terraform state across modules?

---

## Summary

zeropoint-apps treats application lifecycle as **declarative infrastructure**:

- Apps are Terraform modules (developers write `.tf` files)
- Links and exposures are generated Terraform modules
- Terraform handles all dependency resolution, state, and execution
- Service discovery via Docker DNS (no host ports)
- External access via single reverse proxy (Caddy)
- zeropoint is a thin orchestrator that generates modules and invokes Terraform

This architecture is simple, correct, and leverages proven tooling. It avoids reinventing resource graphs, state management, or undo logic — Terraform already does this well.
